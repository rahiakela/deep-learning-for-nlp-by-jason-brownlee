{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_develop_word_based_neural_language_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-5-language-modeling/2_develop_word_based_neural_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop a Word-Based Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "Language modeling involves predicting the next word in a sequence given the sequence of words already present. A language model is a key element in many natural language processing models such as machine translation and speech recognition. The choice of how the language model is framed must match how the language model is intended to be used.\n",
        "\n",
        "Nevertheless, in the field of neural language models, word-based models offer a lot of promise for a general, flexible and powerful approach to language modeling.\n",
        "\n",
        "We will cover the followings topics:-\n",
        "\n",
        "* Developing a good framing of a word-based language model for a given\n",
        "application.\n",
        "* Develop one-word, two-word, and line-based framings for word-based language models.\n",
        "* Generate sequences using a fit language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pickle import dump\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Jack and Jill Nursery Rhyme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "Jack and Jill is a simple nursery rhyme. It is comprised of 4 lines, as follows:\n",
        "\n",
        "Jack and Jill went up the hill\n",
        "To fetch a pail of water\n",
        "Jack fell down and broke his crown\n",
        "And Jill came tumbling after\n",
        "\n",
        "We will use this as our source text for exploring different framings of a word-based language model.\n",
        "\n",
        "```python\n",
        "# source text\n",
        "data = \"\"\" Jack and Jill went up the hill\\n\n",
        "  To fetch a pail of water\\n\n",
        "  Jack fell down and broke his crown\\n\n",
        "  And Jill came tumbling after\\n \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Framing Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "A statistical language model is learned from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. Language models are a key component in larger models for challenging natural language processing problems, like\n",
        "machine translation and speech recognition. They can also be developed as standalone models and used for generating new sequences that have the same statistical properties as the source text.\n",
        "\n",
        "Language models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence. Similarly, when making predictions, the process can be seeded with one or a few words, then predicted words can be gathered and\n",
        "presented as input on subsequent predictions in order to build up a generated output sequence.\n",
        "\n",
        "Therefore, each model will involve splitting the source text into input and output sequences, such that the model can learn to predict words. There are many ways to frame the sequences from a source text for language modeling. We will explore 3 different ways of developing word-based language models in the Keras deep learning library. There is no single best approach, just different framings that may suit different applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "## Model 1: One-Word-In, One-Word-Out Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "We can start with a very simple model. Given one word as input, the model will learn to predict the next word in the sequence.\n",
        "\n",
        "```python\n",
        "X,      y\n",
        "Jack,  and\n",
        "and,   Jill\n",
        "Jill,  went\n",
        "...,   ...\n",
        "```\n",
        "\n",
        "**Step-1**\n",
        "\n",
        "The first step is to encode the text as integers. Each lowercase word in the source text is assigned a unique integer and we can convert the sequences of words to sequences of integers.\n",
        "\n",
        "```python\n",
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "```\n",
        "\n",
        "Keras provides the Tokenizer class that can be used to perform this encoding. First, the Tokenizer is fit on the source text to develop the mapping from words to unique integers. Then sequences of text can be converted to sequences of integers by calling the texts to sequences()\n",
        "function.\n",
        "\n",
        "**Step-2**\n",
        "\n",
        "We will need to know the size of the vocabulary later for both defining the word embedding layer in the model, and for encoding output words using a one hot encoding. The size of the vocabulary can be retrieved from the trained Tokenizer by accessing the word index attribute.\n",
        "\n",
        "```python\n",
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "```\n",
        "\n",
        "The size of the vocabulary is 21 words. We add one, because we will need to specify the integer for the largest encoded word as an array index, e.g.\n",
        "words encoded 1 to 21 with array indicies 0 to 21 or 22 positions. \n",
        "\n",
        "**Step-3**\n",
        "\n",
        "Next, we need to create sequences of words to fit the model with one word as input and one word as output.\n",
        "\n",
        "```python\n",
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "sequence = encoded[i-1:i+1]\n",
        "sequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "```\n",
        "\n",
        "**Step-4**\n",
        "\n",
        "We can then split the sequences into input (X) and output elements (y). This is straightforward as we only have two columns in the data.\n",
        "\n",
        "```python\n",
        "# split into X and y elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "```\n",
        "\n",
        "**Step-5**\n",
        "\n",
        "We will fit our model to predict a probability distribution across all words in the vocabulary. That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value.\n",
        "\n",
        "This gives the network a ground truth to aim for from which we can calculate error and update the model. Keras provides the to categorical() function that we can use to convert theinteger to a one hot encoding while specifying the number of classes as the vocabulary size.\n",
        "\n",
        "```python\n",
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "```\n",
        "\n",
        "**Step-6**\n",
        "\n",
        "We are now ready to define the neural network model. The model uses a learned word embedding in the input layer. This has one real-valued vector for each word in the vocabulary, where each word vector has a specified length. In this case we will use a 10-dimensional projection. The input sequence contains a single word, therefore the input length=1.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(vocab_size):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "  model.add(LSTM(50))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  # compile network\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "Tying all of this together, the complete code listing is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUxIuMFB7Hxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence from the model\n",
        "def generate_seq(model, tokenizer, seed_text, n_words):\n",
        "  in_text, result = seed_text, seed_text\n",
        "\n",
        "  # generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "    # encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = np.array(encoded)\n",
        "\n",
        "    # predict a word in the vocabulary\n",
        "    yhat = model.predict_classes(encoded, verbose=0)\n",
        "    # map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    \n",
        "    in_text, result = out_word, result + ' ' + out_word\n",
        "  return result\n",
        "\n",
        "# define the model\n",
        "def define_model(vocab_size):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "  model.add(LSTM(50))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "\n",
        "# source text\n",
        "data = \"\"\"\n",
        "  Jack and Jill went up the hill\\n\n",
        "  To fetch a pail of water\\n\n",
        "  Jack fell down and broke his crown\\n\n",
        "  And Jill came tumbling after\\n\n",
        "\"\"\"\n",
        "\n",
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary Size: {str(vocab_size)}')\n",
        "\n",
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "  sequence = encoded[i - 1: i + 1]\n",
        "  sequences.append(sequence)\n",
        "print(f'Total Sequences: {str(len(sequences))}')\n",
        "\n",
        "# split into X and y elements\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, 0], sequences[:, 1]\n",
        "\n",
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(vocab_size)\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=500, verbose=2)\n",
        "\n",
        "# evaluate the model\n",
        "print(generate_seq(model, tokenizer, 'Jack', 6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1MEgLhVVzuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cfdd019c-390f-4c34-97fb-40458465dbf0"
      },
      "source": [
        "# evaluate the model\n",
        "print(generate_seq(model, tokenizer, 'And', 6))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And jill went up the hill to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSY22PchWu2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1f37afe4-af3f-47dc-8a2d-18c5aafbbbff"
      },
      "source": [
        "print(generate_seq(model, tokenizer, 'To', 6))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To fetch a pail of water jack\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ymYzzblWQwr",
        "colab_type": "text"
      },
      "source": [
        "This is a good first cut language model, but does not take full advantage of the LSTM's ability to handle sequences of input and disambiguate some of the ambiguous pairwise sequences by using a broader context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR5dPSuHWZih",
        "colab_type": "text"
      },
      "source": [
        "## Model 2: Line-by-Line Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PriJNCxmWcep",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs2jYVJIFMMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}