{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-develop-embedding-cnn-model-for-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-4-text-classi-cation/1_develop_embedding_cnn_model_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop an Embedding + CNN Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "Word embeddings are a technique for representing text where different words with similar meaning have a similar real-valued vector representation. They are a key breakthrough that has led to great performance of neural network models on a suite of challenging natural language processing problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x75Yt91gjqtm",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZsk9bujrOP",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh6TKK0tjtWy",
        "colab_type": "code",
        "outputId": "03047f1e-fe2e-4d2f-8080-f8837d8a487e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 2010, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/2010)\u001b[K\rremote: Counting objects:   1% (21/2010)\u001b[K\rremote: Counting objects:   2% (41/2010)\u001b[K\rremote: Counting objects:   3% (61/2010)\u001b[K\rremote: Counting objects:   4% (81/2010)\u001b[K\rremote: Counting objects:   5% (101/2010)\u001b[K\rremote: Counting objects:   6% (121/2010)\u001b[K\rremote: Counting objects:   7% (141/2010)\u001b[K\rremote: Counting objects:   8% (161/2010)\u001b[K\rremote: Counting objects:   9% (181/2010)\u001b[K\rremote: Counting objects:  10% (201/2010)\u001b[K\rremote: Counting objects:  11% (222/2010)\u001b[K\rremote: Counting objects:  12% (242/2010)\u001b[K\rremote: Counting objects:  13% (262/2010)\u001b[K\rremote: Counting objects:  14% (282/2010)\u001b[K\rremote: Counting objects:  15% (302/2010)\u001b[K\rremote: Counting objects:  16% (322/2010)\u001b[K\rremote: Counting objects:  17% (342/2010)\u001b[K\rremote: Counting objects:  18% (362/2010)\u001b[K\rremote: Counting objects:  19% (382/2010)\u001b[K\rremote: Counting objects:  20% (402/2010)\u001b[K\rremote: Counting objects:  21% (423/2010)\u001b[K\rremote: Counting objects:  22% (443/2010)\u001b[K\rremote: Counting objects:  23% (463/2010)\u001b[K\rremote: Counting objects:  24% (483/2010)\u001b[K\rremote: Counting objects:  25% (503/2010)\u001b[K\rremote: Counting objects:  26% (523/2010)\u001b[K\rremote: Counting objects:  27% (543/2010)\u001b[K\rremote: Counting objects:  28% (563/2010)\u001b[K\rremote: Counting objects:  29% (583/2010)\u001b[K\rremote: Counting objects:  30% (603/2010)\u001b[K\rremote: Counting objects:  31% (624/2010)\u001b[K\rremote: Counting objects:  32% (644/2010)\u001b[K\rremote: Counting objects:  33% (664/2010)\u001b[K\rremote: Counting objects:  34% (684/2010)\u001b[K\rremote: Counting objects:  35% (704/2010)\u001b[K\rremote: Counting objects:  36% (724/2010)\u001b[K\rremote: Counting objects:  37% (744/2010)\u001b[K\rremote: Counting objects:  38% (764/2010)\u001b[K\rremote: Counting objects:  39% (784/2010)\u001b[K\rremote: Counting objects:  40% (804/2010)\u001b[K\rremote: Counting objects:  41% (825/2010)\u001b[K\rremote: Counting objects:  42% (845/2010)\u001b[K\rremote: Counting objects:  43% (865/2010)\u001b[K\rremote: Counting objects:  44% (885/2010)\u001b[K\rremote: Counting objects:  45% (905/2010)\u001b[K\rremote: Counting objects:  46% (925/2010)\u001b[K\rremote: Counting objects:  47% (945/2010)\u001b[K\rremote: Counting objects:  48% (965/2010)\u001b[K\rremote: Counting objects:  49% (985/2010)\u001b[K\rremote: Counting objects:  50% (1005/2010)\u001b[K\rremote: Counting objects:  51% (1026/2010)\u001b[K\rremote: Counting objects:  52% (1046/2010)\u001b[K\rremote: Counting objects:  53% (1066/2010)\u001b[K\rremote: Counting objects:  54% (1086/2010)\u001b[K\rremote: Counting objects:  55% (1106/2010)\u001b[K\rremote: Counting objects:  56% (1126/2010)\u001b[K\rremote: Counting objects:  57% (1146/2010)\u001b[K\rremote: Counting objects:  58% (1166/2010)\u001b[K\rremote: Counting objects:  59% (1186/2010)\u001b[K\rremote: Counting objects:  60% (1206/2010)\u001b[K\rremote: Counting objects:  61% (1227/2010)\u001b[K\rremote: Counting objects:  62% (1247/2010)\u001b[K\rremote: Counting objects:  63% (1267/2010)\u001b[K\rremote: Counting objects:  64% (1287/2010)\u001b[K\rremote: Counting objects:  65% (1307/2010)\u001b[K\rremote: Counting objects:  66% (1327/2010)\u001b[K\rremote: Counting objects:  67% (1347/2010)\u001b[K\rremote: Counting objects:  68% (1367/2010)\u001b[K\rremote: Counting objects:  69% (1387/2010)\u001b[K\rremote: Counting objects:  70% (1407/2010)\u001b[K\rremote: Counting objects:  71% (1428/2010)\u001b[K\rremote: Counting objects:  72% (1448/2010)\u001b[K\rremote: Counting objects:  73% (1468/2010)\u001b[K\rremote: Counting objects:  74% (1488/2010)\u001b[K\rremote: Counting objects:  75% (1508/2010)\u001b[K\rremote: Counting objects:  76% (1528/2010)\u001b[K\rremote: Counting objects:  77% (1548/2010)\u001b[K\rremote: Counting objects:  78% (1568/2010)\u001b[K\rremote: Counting objects:  79% (1588/2010)\u001b[K\rremote: Counting objects:  80% (1608/2010)\u001b[K\rremote: Counting objects:  81% (1629/2010)\u001b[K\rremote: Counting objects:  82% (1649/2010)\u001b[K\rremote: Counting objects:  83% (1669/2010)\u001b[K\rremote: Counting objects:  84% (1689/2010)\u001b[K\rremote: Counting objects:  85% (1709/2010)\u001b[K\rremote: Counting objects:  86% (1729/2010)\u001b[K\rremote: Counting objects:  87% (1749/2010)\u001b[K\rremote: Counting objects:  88% (1769/2010)\u001b[K\rremote: Counting objects:  89% (1789/2010)\u001b[K\rremote: Counting objects:  90% (1809/2010)\u001b[K\rremote: Counting objects:  91% (1830/2010)\u001b[K\rremote: Counting objects:  92% (1850/2010)\u001b[K\rremote: Counting objects:  93% (1870/2010)\u001b[K\rremote: Counting objects:  94% (1890/2010)\u001b[K\rremote: Counting objects:  95% (1910/2010)\u001b[K\rremote: Counting objects:  96% (1930/2010)\u001b[K\rremote: Counting objects:  97% (1950/2010)\u001b[K\rremote: Counting objects:  98% (1970/2010)\u001b[K\rremote: Counting objects:  99% (1990/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/2009)\u001b[K\rremote: Compressing objects:   1% (21/2009)\u001b[K\rremote: Compressing objects:   2% (41/2009)\u001b[K\rremote: Compressing objects:   3% (61/2009)\u001b[K\rremote: Compressing objects:   4% (81/2009)\u001b[K\rremote: Compressing objects:   5% (101/2009)\u001b[K\rremote: Compressing objects:   6% (121/2009)\u001b[K\rremote: Compressing objects:   7% (141/2009)\u001b[K\rremote: Compressing objects:   8% (161/2009)\u001b[K\rremote: Compressing objects:   9% (181/2009)\u001b[K\rremote: Compressing objects:  10% (201/2009)\u001b[K\rremote: Compressing objects:  11% (221/2009)\u001b[K\rremote: Compressing objects:  12% (242/2009)\u001b[K\rremote: Compressing objects:  13% (262/2009)\u001b[K\rremote: Compressing objects:  14% (282/2009)\u001b[K\rremote: Compressing objects:  15% (302/2009)\u001b[K\rremote: Compressing objects:  16% (322/2009)\u001b[K\rremote: Compressing objects:  17% (342/2009)\u001b[K\rremote: Compressing objects:  18% (362/2009)\u001b[K\rremote: Compressing objects:  19% (382/2009)\u001b[K\rremote: Compressing objects:  20% (402/2009)\u001b[K\rremote: Compressing objects:  21% (422/2009)\u001b[K\rremote: Compressing objects:  22% (442/2009)\u001b[K\rremote: Compressing objects:  23% (463/2009)\u001b[K\rremote: Compressing objects:  24% (483/2009)\u001b[K\rremote: Compressing objects:  25% (503/2009)\u001b[K\rremote: Compressing objects:  26% (523/2009)\u001b[K\rremote: Compressing objects:  27% (543/2009)\u001b[K\rremote: Compressing objects:  28% (563/2009)\u001b[K\rremote: Compressing objects:  29% (583/2009)\u001b[K\rremote: Compressing objects:  30% (603/2009)\u001b[K\rremote: Compressing objects:  31% (623/2009)\u001b[K\rremote: Compressing objects:  32% (643/2009)\u001b[K\rremote: Compressing objects:  33% (663/2009)\u001b[K\rremote: Compressing objects:  34% (684/2009)\u001b[K\rremote: Compressing objects:  35% (704/2009)\u001b[K\rremote: Compressing objects:  36% (724/2009)\u001b[K\rremote: Compressing objects:  37% (744/2009)\u001b[K\rremote: Compressing objects:  38% (764/2009)\u001b[K\rremote: Compressing objects:  39% (784/2009)\u001b[K\rremote: Compressing objects:  40% (804/2009)\u001b[K\rremote: Compressing objects:  41% (824/2009)\u001b[K\rremote: Compressing objects:  42% (844/2009)\u001b[K\rremote: Compressing objects:  43% (864/2009)\u001b[K\rremote: Compressing objects:  44% (884/2009)\u001b[K\rremote: Compressing objects:  45% (905/2009)\u001b[K\rremote: Compressing objects:  46% (925/2009)\u001b[K\rremote: Compressing objects:  47% (945/2009)\u001b[K\rremote: Compressing objects:  48% (965/2009)\u001b[K\rremote: Compressing objects:  49% (985/2009)\u001b[K\rremote: Compressing objects:  50% (1005/2009)\u001b[K\rremote: Compressing objects:  51% (1025/2009)\u001b[K\rremote: Compressing objects:  52% (1045/2009)\u001b[K\rremote: Compressing objects:  53% (1065/2009)\u001b[K\rremote: Compressing objects:  54% (1085/2009)\u001b[K\rremote: Compressing objects:  55% (1105/2009)\u001b[K\rremote: Compressing objects:  56% (1126/2009)\u001b[K\rremote: Compressing objects:  57% (1146/2009)\u001b[K\rremote: Compressing objects:  58% (1166/2009)\u001b[K\rremote: Compressing objects:  59% (1186/2009)\u001b[K\rremote: Compressing objects:  60% (1206/2009)\u001b[K\rremote: Compressing objects:  61% (1226/2009)\u001b[K\rremote: Compressing objects:  62% (1246/2009)\u001b[K\rremote: Compressing objects:  63% (1266/2009)\u001b[K\rremote: Compressing objects:  64% (1286/2009)\u001b[K\rremote: Compressing objects:  65% (1306/2009)\u001b[K\rremote: Compressing objects:  66% (1326/2009)\u001b[K\rremote: Compressing objects:  67% (1347/2009)\u001b[K\rremote: Compressing objects:  68% (1367/2009)\u001b[K\rremote: Compressing objects:  69% (1387/2009)\u001b[K\rremote: Compressing objects:  70% (1407/2009)\u001b[K\rremote: Compressing objects:  71% (1427/2009)\u001b[K\rremote: Compressing objects:  72% (1447/2009)\u001b[K\rremote: Compressing objects:  73% (1467/2009)\u001b[K\rremote: Compressing objects:  74% (1487/2009)\u001b[K\rremote: Compressing objects:  75% (1507/2009)\u001b[K\rremote: Compressing objects:  76% (1527/2009)\u001b[K\rremote: Compressing objects:  77% (1547/2009)\u001b[K\rremote: Compressing objects:  78% (1568/2009)\u001b[K\rremote: Compressing objects:  79% (1588/2009)\u001b[K\rremote: Compressing objects:  80% (1608/2009)\u001b[K\rremote: Compressing objects:  81% (1628/2009)\u001b[K\rremote: Compressing objects:  82% (1648/2009)\u001b[K\rremote: Compressing objects:  83% (1668/2009)\u001b[K\rremote: Compressing objects:  84% (1688/2009)\u001b[K\rremote: Compressing objects:  85% (1708/2009)\u001b[K\rremote: Compressing objects:  86% (1728/2009)\u001b[K\rremote: Compressing objects:  87% (1748/2009)\u001b[K\rremote: Compressing objects:  88% (1768/2009)\u001b[K\rremote: Compressing objects:  89% (1789/2009)\u001b[K\rremote: Compressing objects:  90% (1809/2009)\u001b[K\rremote: Compressing objects:  91% (1829/2009)\u001b[K\rremote: Compressing objects:  92% (1849/2009)\u001b[K\rremote: Compressing objects:  93% (1869/2009)\u001b[K\rremote: Compressing objects:  94% (1889/2009)\u001b[K\rremote: Compressing objects:  95% (1909/2009)\u001b[K\rremote: Compressing objects:  96% (1929/2009)\u001b[K\rremote: Compressing objects:  97% (1949/2009)\u001b[K\rremote: Compressing objects:  98% (1969/2009)\u001b[K\rremote: Compressing objects:  99% (1989/2009)\u001b[K\rremote: Compressing objects: 100% (2009/2009)\u001b[K\rremote: Compressing objects: 100% (2009/2009), done.\u001b[K\n",
            "Receiving objects:   0% (1/2010)   \rReceiving objects:   1% (21/2010)   \rReceiving objects:   2% (41/2010)   \rReceiving objects:   3% (61/2010)   \rReceiving objects:   4% (81/2010)   \rReceiving objects:   5% (101/2010)   \rReceiving objects:   6% (121/2010)   \rReceiving objects:   7% (141/2010)   \rReceiving objects:   8% (161/2010)   \rReceiving objects:   9% (181/2010)   \rReceiving objects:  10% (201/2010)   \rReceiving objects:  11% (222/2010)   \rReceiving objects:  12% (242/2010)   \rReceiving objects:  13% (262/2010)   \rReceiving objects:  14% (282/2010)   \rReceiving objects:  15% (302/2010)   \rReceiving objects:  16% (322/2010)   \rReceiving objects:  17% (342/2010)   \rReceiving objects:  18% (362/2010)   \rReceiving objects:  19% (382/2010)   \rReceiving objects:  20% (402/2010)   \rReceiving objects:  21% (423/2010)   \rReceiving objects:  22% (443/2010)   \rReceiving objects:  23% (463/2010)   \rReceiving objects:  24% (483/2010)   \rReceiving objects:  25% (503/2010)   \rReceiving objects:  26% (523/2010)   \rReceiving objects:  27% (543/2010)   \rReceiving objects:  28% (563/2010)   \rReceiving objects:  29% (583/2010)   \rReceiving objects:  30% (603/2010)   \rReceiving objects:  31% (624/2010)   \rReceiving objects:  32% (644/2010)   \rReceiving objects:  33% (664/2010)   \rReceiving objects:  34% (684/2010)   \rReceiving objects:  35% (704/2010)   \rReceiving objects:  36% (724/2010)   \rReceiving objects:  37% (744/2010)   \rReceiving objects:  38% (764/2010)   \rReceiving objects:  39% (784/2010)   \rReceiving objects:  40% (804/2010)   \rReceiving objects:  41% (825/2010)   \rReceiving objects:  42% (845/2010)   \rReceiving objects:  43% (865/2010)   \rReceiving objects:  44% (885/2010)   \rReceiving objects:  45% (905/2010)   \rReceiving objects:  46% (925/2010)   \rReceiving objects:  47% (945/2010)   \rReceiving objects:  48% (965/2010)   \rReceiving objects:  49% (985/2010)   \rReceiving objects:  50% (1005/2010)   \rReceiving objects:  51% (1026/2010)   \rReceiving objects:  52% (1046/2010)   \rReceiving objects:  53% (1066/2010)   \rReceiving objects:  54% (1086/2010)   \rReceiving objects:  55% (1106/2010)   \rReceiving objects:  56% (1126/2010)   \rReceiving objects:  57% (1146/2010)   \rReceiving objects:  58% (1166/2010)   \rReceiving objects:  59% (1186/2010)   \rReceiving objects:  60% (1206/2010)   \rReceiving objects:  61% (1227/2010)   \rReceiving objects:  62% (1247/2010)   \rReceiving objects:  63% (1267/2010)   \rReceiving objects:  64% (1287/2010)   \rReceiving objects:  65% (1307/2010)   \rReceiving objects:  66% (1327/2010)   \rReceiving objects:  67% (1347/2010)   \rReceiving objects:  68% (1367/2010)   \rReceiving objects:  69% (1387/2010)   \rReceiving objects:  70% (1407/2010)   \rReceiving objects:  71% (1428/2010)   \rReceiving objects:  72% (1448/2010)   \rReceiving objects:  73% (1468/2010)   \rReceiving objects:  74% (1488/2010)   \rReceiving objects:  75% (1508/2010)   \rReceiving objects:  76% (1528/2010)   \rReceiving objects:  77% (1548/2010)   \rReceiving objects:  78% (1568/2010)   \rReceiving objects:  79% (1588/2010)   \rReceiving objects:  80% (1608/2010)   \rReceiving objects:  81% (1629/2010)   \rReceiving objects:  82% (1649/2010)   \rReceiving objects:  83% (1669/2010)   \rReceiving objects:  84% (1689/2010)   \rReceiving objects:  85% (1709/2010)   \rReceiving objects:  86% (1729/2010)   \rReceiving objects:  87% (1749/2010)   \rReceiving objects:  88% (1769/2010)   \rReceiving objects:  89% (1789/2010)   \rReceiving objects:  90% (1809/2010)   \rReceiving objects:  91% (1830/2010)   \rReceiving objects:  92% (1850/2010)   \rReceiving objects:  93% (1870/2010)   \rReceiving objects:  94% (1890/2010)   \rReceiving objects:  95% (1910/2010)   \rReceiving objects:  96% (1930/2010)   \rReceiving objects:  97% (1950/2010)   \rReceiving objects:  98% (1970/2010)   \rremote: Total 2010 (delta 1), reused 2009 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects:  99% (1990/2010)   \rReceiving objects: 100% (2010/2010)   \rReceiving objects: 100% (2010/2010), 3.55 MiB | 19.35 MiB/s, done.\n",
            "Resolving deltas:   0% (0/1)   \rResolving deltas: 100% (1/1)   \rResolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at 3 things:\n",
        "\n",
        "1.   Separation of data into training and test sets.\n",
        "2.   Loading and cleaning the data to remove punctuation and numbers.\n",
        "3.   Defining a vocabulary of preferred words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative. This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model.\n",
        "\n",
        "We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the\n",
        "test set that could help us better prepare the data (e.g. the words used) is unavailable during the preparation of data and the training of the model. \n",
        "\n",
        "That being said, we will use the last 100 positive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for testing the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CSD7eIlEul",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Cleaning Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "The text data is already pretty clean, so not much preparation is required. Without getting too much into the details, we will prepare the data using the following method:\n",
        "\n",
        "* Split tokens on white space.\n",
        "* Remove all punctuation from words.\n",
        "* Remove all words that are not purely comprised of alphabetical characters.\n",
        "* Remove all words that are known stop words.\n",
        "* Remove all words that have a length <= 1 character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwd9MUaDoLDt",
        "colab_type": "code",
        "outputId": "2c0c9b7a-1eb0-4123-d8ed-6925fe1150c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqZugLnjz59",
        "colab_type": "code",
        "outputId": "77206617-fa49-4ee3-b4f5-0e3d85b36ea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# define base path for dataset\n",
        "base_path = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken'\n",
        "\n",
        "# load one file\n",
        "filename = base_path + '/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# clean doc\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Define a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "It is important to define a vocabulary of known words when using a bag-of-words model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
        "\n",
        "This is dificult to know beforehand and often it is important to test difierent hypotheses about how to construct a useful vocabulary. We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the counter and we can step over all of the reviews in the negative directory and then the positive directory.\n",
        "\n",
        "```python\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhSeu-kGqem3",
        "colab_type": "text"
      },
      "source": [
        "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
        "```python\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c > min_occurane]\n",
        "print(len(tokens))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "Finally, the vocabulary can be saved to a new file called vocab.txt that we can later load and use to filter movie reviews prior to encoding them for modeling.\n",
        "```python\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "outputId": "78d87ede-c1ba-483a-f908-0842aa01babc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9a4PZI02Qs",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to look at extracting features from the reviews ready for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Train CNN With Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will learn a word embedding while training a convolutional neural network on the classification problem. A word embedding is a way of representing text where each word in the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space (close in the vector space). This is a more expressive representation for text than more classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches.\n",
        "\n",
        "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer. The first step is to load the vocabulary. We will use it to filter out words from movie reviews that\n",
        "we are not interested in.\n",
        "\n",
        "So we have a local file called vocab.txt with one word per line. We can load that file and build a vocabulary as a set for checking the validity of tokens.\n",
        "\n",
        "```python\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "```\n",
        "\n",
        "Next, we need to load all of the training data movie reviews. For that we can adapt the process docs() to load the documents, clean them, and return them as a list of strings, with one document per string. We want each document to be a string for easy encoding as a sequence of integers later. Cleaning the document involves splitting each review based on white space, removing punctuation, and then filtering out all tokens not in the vocabulary.\n",
        "\n",
        "```python\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "```\n",
        "\n",
        "We can call the process docs function for both the neg and pos directories and combine the reviews into a single train or test dataset. We also can define the class labels for the dataset.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab)\n",
        "  pos = process_docs(base_path + '/pos', vocab)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHgm9Hqukqlg",
        "colab_type": "text"
      },
      "source": [
        "### Movie Reviews to Bag-of-Words Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znsuEf27krr8",
        "colab_type": "text"
      },
      "source": [
        "We will use the Keras API to convert reviews to encoded document vectors. Keras provides the Tokenizer class that can do some of the cleaning and vocab definition tasks.It is better to do this ourselves to know exactly what was done and why. \n",
        "\n",
        "Nevertheless, the Tokenizer class is convenient and will easily transform documents into encoded vectors. First, the Tokenizer must be created, then fit on the text documents in the training dataset. In this case, these are the aggregation of the positive lines and negative lines arrays.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```\n",
        "\n",
        "This process determines a consistent way to convert the vocabulary to a fixed-length vector with 25,768 elements, which is the total number of words in the vocabulary file vocab.txt.\n",
        "\n",
        "Next, documents can then be encoded using the Tokenizer by calling texts to matrix(). The function takes both a list of documents to encode and an encoding mode, which is the method\n",
        "used to score words in the document. Here we specify freq to score words based on their frequency in the document. This can be used to encode the loaded training and test data, for example.\n",
        "\n",
        "```python\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "```\n",
        "This encodes all of the positive and negative reviews in the training dataset.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "```\n",
        "\n",
        "Similarly, the load clean dataset() dataset must be updated to load either train or test data.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "outputId": "b0402dde-5b9d-4d0a-d0ed-ab12863222cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "print(len(train_docs))\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1800\n",
            "(1800, 25768) (200, 25768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkeDOIHF5WRO",
        "colab_type": "text"
      },
      "source": [
        "The shape of the encoded training dataset and test dataset\n",
        "with 1,800 and 200 documents respectively, each with the same sized encoding vocabulary (vector length)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBg8wz25chd",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvlF4AgaJBfL",
        "colab_type": "text"
      },
      "source": [
        "we will develop Multilayer Perceptron (MLP) models to classify encoded documents as either positive or negative. The models will be simple feedforward network models with fully connected layers called Dense in the Keras deep learning library. This section is divided into 3 sections:\n",
        "\n",
        "\n",
        "1.   First sentiment analysis model\n",
        "2.   Comparing word scoring modes\n",
        "3.   Making a prediction for new reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ZL3A6qJTU_",
        "colab_type": "text"
      },
      "source": [
        "### First Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDWjF2rkJUeW",
        "colab_type": "text"
      },
      "source": [
        "We can develop a simple MLP model to predict the sentiment of encoded reviews. The model will have an input layer that equals the number of words in the vocabulary, and in turn the length of the input documents. We can store this in a new variable called n words, as follows:\n",
        "\n",
        "```python\n",
        "n_words = Xtest.shape[1]\n",
        "```\n",
        "\n",
        "We can now define the network. All model configuration was found with very little trial and error and should not be considered tuned for this problem. \n",
        "\n",
        "We will use a single hidden layer with 50 neurons and a rectified linear activation function. The output layer is a single neuron with a sigmoid activation function for predicting 0 for negative and 1 for positive reviews. \n",
        "\n",
        "The network will be trained using the efficient Adam implementation of gradient descent and thecbinary cross entropy loss function, suited to binary classification problems. We will keep track of accuracy when training and evaluating the model.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "  # define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "```\n",
        "\n",
        "Next, we can fit the model on the training data; in this case, the model is small and is easily fit in 10 epochs.\n",
        "\n",
        "```python\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "```\n",
        "\n",
        "Finally, once the model is trained, we can evaluate its performance by making predictions in the test dataset and printing the accuracy.\n",
        "\n",
        "```python\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "```\n",
        "\n",
        "The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARD7nUYgtrR-",
        "colab_type": "code",
        "outputId": "4d04742a-0b4a-44ca-f1d0-b284bb7accb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "  # define network\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Dense(50, input_shape=(n_words, ), activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "\n",
        "  # plot the model structure\n",
        "  keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "print(len(train_docs))\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)\n",
        "\n",
        "# define the model\n",
        "print(Xtest.shape)\n",
        "n_words = Xtest.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# ref: https://stackoverflow.com/questions/58682026/failed-to-find-data-adapter-that-can-handle-input-class-numpy-ndarray-cl\n",
        "# convert array to numpy array\n",
        "Xtrain = np.array(Xtrain)\n",
        "ytrain = np.array(ytrain)\n",
        "Xtest = np.array(Xtest)\n",
        "ytest = np.array(ytest)\n",
        "\n",
        "# fit the model\n",
        "history = model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "# evaluate the model \n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print(f'Test Accuracy: {str(acc * 100)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1800\n",
            "(1800, 25768) (200, 25768)\n",
            "(200, 25768)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1800 samples\n",
            "Epoch 1/10\n",
            "1800/1800 - 2s - loss: 0.6916 - accuracy: 0.5583\n",
            "Epoch 2/10\n",
            "1800/1800 - 1s - loss: 0.6832 - accuracy: 0.6356\n",
            "Epoch 3/10\n",
            "1800/1800 - 1s - loss: 0.6671 - accuracy: 0.8694\n",
            "Epoch 4/10\n",
            "1800/1800 - 1s - loss: 0.6429 - accuracy: 0.8722\n",
            "Epoch 5/10\n",
            "1800/1800 - 1s - loss: 0.6108 - accuracy: 0.9039\n",
            "Epoch 6/10\n",
            "1800/1800 - 1s - loss: 0.5710 - accuracy: 0.9400\n",
            "Epoch 7/10\n",
            "1800/1800 - 1s - loss: 0.5286 - accuracy: 0.9483\n",
            "Epoch 8/10\n",
            "1800/1800 - 1s - loss: 0.4850 - accuracy: 0.9600\n",
            "Epoch 9/10\n",
            "1800/1800 - 1s - loss: 0.4428 - accuracy: 0.9600\n",
            "Epoch 10/10\n",
            "1800/1800 - 1s - loss: 0.4015 - accuracy: 0.9661\n",
            "Test Accuracy: 87.00000047683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hlV59UCQuG_",
        "colab_type": "text"
      },
      "source": [
        "We can see that the model easily fits the training data within the 10 epochs, achieving close to 100% accuracy. Evaluating the model on the test dataset, we can see that model does well,\n",
        "achieving an accuracy of above 87%, well within the ballpark of low-to-mid 80s seen in the original paper. Although, it is important to note that this is not an apples-to-apples comparison, as the original paper used 10-fold cross-validation to estimate model skill instead of a single train/test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEyf2iEgRObP",
        "colab_type": "code",
        "outputId": "8ec10a63-8dd0-4f64-e174-6845719213f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training and loss accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94pqibqeQ6hJ",
        "colab_type": "text"
      },
      "source": [
        "### Comparing Word Scoring Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZv2eC17Q7iN",
        "colab_type": "text"
      },
      "source": [
        "The texts to matrix() function for the Tokenizer in the Keras API provides 4 different methods for scoring words; they are:\n",
        "\n",
        "* **binary**:Where words are marked as present (1) or absent (0).\n",
        "* **count**:Where the occurrence count for each word is marked as an integer.\n",
        "* **tfidf**:Where each word is scored based on their frequency, where words that are common across all documents are penalized.\n",
        "* **freq**:Where words are scored based on their frequency of occurrence within the document.\n",
        "\n",
        "We can evaluate the skill of the model developed in the previous section fit using each of the 4 supported word scoring modes. This first involves the development of a function to create an\n",
        "encoding of the loaded documents based on a chosen scoring model. The function creates the tokenizer, fits it on the training documents, then creates the train and test encodings using the chosen model.\n",
        "\n",
        "```python\n",
        "# prepare bag-of-words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "  # create the tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  # fit the tokenizer on the documents\n",
        "  tokenizer.fit_on_texts(train_docs)\n",
        "  # encode training data set\n",
        "  Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "  # encode training data set\n",
        "  Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "  return Xtrain, Xtest\n",
        "```\n",
        "\n",
        "We also need a function to evaluate the MLP given a specific encoding of the data. Because neural networks are stochastic, they can produce different results when the same model is fit on\n",
        "the same data. This is mainly because of the random initial weights and the shuffling of patterns during mini-batch gradient descent. This means that any one scoring of a model is unreliable\n",
        "and we should estimate model skill based on an average of multiple runs.\n",
        "\n",
        "```python\n",
        "# evaluate a neural network model\n",
        "def evaluate_model(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores = list()\n",
        "  n_repeats = 30\n",
        "  n_words = Xtest.shape[1]\n",
        "  for i in range(n_repeats):\n",
        "  # define network\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "  # evaluate\n",
        "  loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "  scores.append(acc)\n",
        "  print('%d accuracy: %s' % ((i+1), acc))\n",
        "  return scores\n",
        "```\n",
        "\n",
        "Pulling all of this together, the complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3gogVFUOIQe",
        "colab_type": "code",
        "outputId": "97db7125-0466-4366-c82d-e396344d58ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_model(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores = list()\n",
        "  n_repeats = 10\n",
        "  print(Xtest.shape)\n",
        "  n_words = Xtest.shape[1]\n",
        "\n",
        "  for i in range(n_repeats):\n",
        "    # define network\n",
        "    model = define_model(n_words)\n",
        "    # fit network\n",
        "    model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "    # evaluate the model\n",
        "    _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "    scores.append(acc)\n",
        "    print(f'{str(i + 1)} accuracy: {str(acc)}')\n",
        "  return scores\n",
        "\n",
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "  # create the tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  # fit the tokenizer on the documents\n",
        "  tokenizer.fit_on_texts(train_docs)\n",
        "  # encode training data set\n",
        "  Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "  # encode test data set\n",
        "  Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "  \n",
        "  return Xtrain, Xtest\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# convert array to numpy array\n",
        "ytrain = np.array(ytrain)\n",
        "ytest = np.array(ytest)\n",
        "\n",
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "  # prepare data for mode\n",
        "  Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "  print(Xtrain.shape, Xtest.shape)\n",
        "  # convert array to numpy array\n",
        "  Xtrain = np.array(Xtrain)\n",
        "  Xtest = np.array(Xtest)\n",
        "  print(Xtrain.shape, Xtest.shape)\n",
        "  # evaluate model on data for mode\n",
        "  results[mode] = evaluate_model(Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "\n",
        "# plot results\n",
        "results.boxplot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 25768) (200, 25768)\n",
            "(1800, 25768) (200, 25768)\n",
            "(200, 25768)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1 accuracy: 0.92\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2 accuracy: 0.92\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "3 accuracy: 0.925\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "4 accuracy: 0.92\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "5 accuracy: 0.92\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6 accuracy: 0.94\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "7 accuracy: 0.93\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "8 accuracy: 0.935\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "9 accuracy: 0.915\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10 accuracy: 0.93\n",
            "(1800, 25768) (200, 25768)\n",
            "(1800, 25768) (200, 25768)\n",
            "(200, 25768)\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_22 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1 accuracy: 0.895\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_24 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2 accuracy: 0.9\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_26 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "3 accuracy: 0.9\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "4 accuracy: 0.9\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_30 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "5 accuracy: 0.895\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6 accuracy: 0.885\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_34 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "7 accuracy: 0.895\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "8 accuracy: 0.9\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_38 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "9 accuracy: 0.895\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_40 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10 accuracy: 0.895\n",
            "(1800, 25768) (200, 25768)\n",
            "(1800, 25768) (200, 25768)\n",
            "(200, 25768)\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_42 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1 accuracy: 0.87\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_44 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2 accuracy: 0.885\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_46 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "3 accuracy: 0.875\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "4 accuracy: 0.875\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_50 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "5 accuracy: 0.885\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_52 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6 accuracy: 0.87\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_54 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "7 accuracy: 0.87\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_56 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "8 accuracy: 0.895\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_58 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "9 accuracy: 0.88\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_60 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10 accuracy: 0.88\n",
            "(1800, 25768) (200, 25768)\n",
            "(1800, 25768) (200, 25768)\n",
            "(200, 25768)\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_62 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1 accuracy: 0.865\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_64 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2 accuracy: 0.865\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "3 accuracy: 0.865\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_68 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "4 accuracy: 0.87\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_70 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "5 accuracy: 0.87\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6 accuracy: 0.87\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_74 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "7 accuracy: 0.87\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "8 accuracy: 0.88\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_78 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "9 accuracy: 0.87\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_80 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10 accuracy: 0.87\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.925500   0.896000   0.878500   0.869500\n",
            "std     0.007976   0.004595   0.008182   0.004378\n",
            "min     0.915000   0.885000   0.870000   0.865000\n",
            "25%     0.920000   0.895000   0.871250   0.866250\n",
            "50%     0.922500   0.895000   0.877500   0.870000\n",
            "75%     0.930000   0.900000   0.883750   0.870000\n",
            "max     0.940000   0.900000   0.895000   0.880000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc+riXnLCP0AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y0GhqLKjx6C",
        "colab_type": "text"
      },
      "source": [
        "At the end of the run, summary statistics for each word scoring method are provided, summarizing the distribution of model skill scores across each of the 10 runs per mode. \n",
        "\n",
        "We can see that the mean score of both the count and binary methods appear to be better than freq and tfidf.\n",
        "\n",
        "A box and whisker plot of the results is also presented, summarizing the accuracy distributions per configuration. We can see that binary achieved the best results with a modest spread and might be the preferred approach for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ppdOG6lGG2",
        "colab_type": "text"
      },
      "source": [
        "## Predicting Sentiment for New Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXAf9uMylIQA",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can develop and use a final model to make predictions for new textual reviews. This is why we wanted the model in the first place. First we will train a final model on all of the available data. We will use the binary mode for scoring the bag-of-words model that was shown to give the best results in the previous section.\n",
        "\n",
        "Predicting the sentiment of new reviews involves following the same steps used to prepare the test data. Specifically, loading the text, cleaning the document, filtering tokens by the\n",
        "chosen vocabulary, converting the remaining tokens to a line, encoding it using the Tokenizer, and making a prediction. We can make a prediction of a class value directly with the fit model\n",
        "by calling predict() that will return an integer of 0 for a negative review and 1 for a positive review.\n",
        "\n",
        "```python\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  # clean\n",
        "  tokens = clean_doc(review)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # convert to line\n",
        "  line = ' '.join(tokens)\n",
        "  # encode\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n",
        "```\n",
        "\n",
        "We can now make predictions for new review texts. Below is an example with both a clearly positive and a clearly negative review using the simple MLP developed above with the frequency\n",
        "word scoring mode.\n",
        "\n",
        "```python\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "```\n",
        "\n",
        "Pulling this all together, the complete example for making predictions for new reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXK62zyTeZ95",
        "colab_type": "code",
        "outputId": "7c15cb23-6d1e-48c5-ad3d-026f83626e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab)\n",
        "  pos = process_docs(base_path + '/pos', vocab)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  # clean doc\n",
        "  tokens = clean_doc(review)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  # convert to line\n",
        "  line = ' '.join(tokens)\n",
        "  # encode \n",
        "  encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0, 0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1 - percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab)\n",
        "test_docs, ytest = load_clean_dataset(vocab)\n",
        "print(len(train_docs))\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "print(Xtrain.shape, Xtest.shape)\n",
        "\n",
        "# define the model\n",
        "print(Xtest.shape)\n",
        "n_words = Xtest.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# https://stackoverflow.com/questions/58682026/failed-to-find-data-adapter-that-can-handle-input-class-numpy-ndarray-cl\n",
        "# convert array to numpy array\n",
        "Xtrain = np.array(Xtrain)\n",
        "ytrain = np.array(ytrain)\n",
        "Xtest = np.array(Xtest)\n",
        "ytest = np.array(ytest)\n",
        "\n",
        "# fit the model\n",
        "history = model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print(f'Review: {text}\\n Sentiment: {sentiment} {str(percent * 100)}')\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print(f'Review: {text}\\n Sentiment: {sentiment} {str(percent * 100)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "(2000, 25768) (2000, 25768)\n",
            "(2000, 25768)\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_82 (Dense)             (None, 50)                1288450   \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2000 samples\n",
            "Epoch 1/10\n",
            "2000/2000 - 1s - loss: 0.4806 - accuracy: 0.7685\n",
            "Epoch 2/10\n",
            "2000/2000 - 1s - loss: 0.0654 - accuracy: 0.9920\n",
            "Epoch 3/10\n",
            "2000/2000 - 1s - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "2000/2000 - 1s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "2000/2000 - 1s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "2000/2000 - 1s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "2000/2000 - 1s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "2000/2000 - 1s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "2000/2000 - 1s - loss: 7.7924e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "2000/2000 - 1s - loss: 5.8871e-04 - accuracy: 1.0000\n",
            "Review: Best movie ever! It was great, I recommend it.\n",
            " Sentiment: POSITIVE 54.22459840774536\n",
            "Review: This is a bad movie.\n",
            " Sentiment: NEGATIVE 64.16163146495819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOT68T2alS3h",
        "colab_type": "text"
      },
      "source": [
        "Ideally, we would fit the model on all available data (train and test) to create a final model and save the model and tokenizer to file so that they can be loaded and used in new software."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCmHY6pJlcEe",
        "colab_type": "text"
      },
      "source": [
        "## Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwHgeTholc7i",
        "colab_type": "text"
      },
      "source": [
        "1- **Tune the Network Topology**. Explore alternate network topologies such as deeper or wider networks. Perhaps you can get better performance with a more suited network.\n",
        "\n",
        "2- **Use Regularization.** Explore the use of regularization techniques, such as dropout.Perhaps you can delay the convergence of the model and achieve better test set performance.\n",
        "\n",
        "3- **Training Diagnostics.** Use the test dataset as a validation dataset during training and create plots of train and test loss. Use these diagnostics to tune the batch size and number of training epochs.\n",
        "\n",
        "4- **Use Bigrams.** Prepare the model to score bigrams of words and evaluate the performance under different scoring schemes.\n",
        "\n"
      ]
    }
  ]
}