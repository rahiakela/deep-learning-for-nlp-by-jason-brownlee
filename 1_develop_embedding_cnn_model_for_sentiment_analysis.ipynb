{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-develop-embedding-cnn-model-for-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-4-text-classi-cation/1_develop_embedding_cnn_model_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop an Embedding + CNN Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "Word embeddings are a technique for representing text where different words with similar meaning have a similar real-valued vector representation. They are a key breakthrough that has led to great performance of neural network models on a suite of challenging natural language processing problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a16d4ce4-0a78-40a9-fb65-6a1f079d3cd3"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Conv1D, MaxPool1D\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x75Yt91gjqtm",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZsk9bujrOP",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh6TKK0tjtWy",
        "colab_type": "code",
        "outputId": "126ad1a1-ce31-4929-959e-a7fafd169bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 2010, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/2010)\u001b[K\rremote: Counting objects:   1% (21/2010)\u001b[K\rremote: Counting objects:   2% (41/2010)\u001b[K\rremote: Counting objects:   3% (61/2010)\u001b[K\rremote: Counting objects:   4% (81/2010)\u001b[K\rremote: Counting objects:   5% (101/2010)\u001b[K\rremote: Counting objects:   6% (121/2010)\u001b[K\rremote: Counting objects:   7% (141/2010)\u001b[K\rremote: Counting objects:   8% (161/2010)\u001b[K\rremote: Counting objects:   9% (181/2010)\u001b[K\rremote: Counting objects:  10% (201/2010)\u001b[K\rremote: Counting objects:  11% (222/2010)\u001b[K\rremote: Counting objects:  12% (242/2010)\u001b[K\rremote: Counting objects:  13% (262/2010)\u001b[K\rremote: Counting objects:  14% (282/2010)\u001b[K\rremote: Counting objects:  15% (302/2010)\u001b[K\rremote: Counting objects:  16% (322/2010)\u001b[K\rremote: Counting objects:  17% (342/2010)\u001b[K\rremote: Counting objects:  18% (362/2010)\u001b[K\rremote: Counting objects:  19% (382/2010)\u001b[K\rremote: Counting objects:  20% (402/2010)\u001b[K\rremote: Counting objects:  21% (423/2010)\u001b[K\rremote: Counting objects:  22% (443/2010)\u001b[K\rremote: Counting objects:  23% (463/2010)\u001b[K\rremote: Counting objects:  24% (483/2010)\u001b[K\rremote: Counting objects:  25% (503/2010)\u001b[K\rremote: Counting objects:  26% (523/2010)\u001b[K\rremote: Counting objects:  27% (543/2010)\u001b[K\rremote: Counting objects:  28% (563/2010)\u001b[K\rremote: Counting objects:  29% (583/2010)\u001b[K\rremote: Counting objects:  30% (603/2010)\u001b[K\rremote: Counting objects:  31% (624/2010)\u001b[K\rremote: Counting objects:  32% (644/2010)\u001b[K\rremote: Counting objects:  33% (664/2010)\u001b[K\rremote: Counting objects:  34% (684/2010)\u001b[K\rremote: Counting objects:  35% (704/2010)\u001b[K\rremote: Counting objects:  36% (724/2010)\u001b[K\rremote: Counting objects:  37% (744/2010)\u001b[K\rremote: Counting objects:  38% (764/2010)\u001b[K\rremote: Counting objects:  39% (784/2010)\u001b[K\rremote: Counting objects:  40% (804/2010)\u001b[K\rremote: Counting objects:  41% (825/2010)\u001b[K\rremote: Counting objects:  42% (845/2010)\u001b[K\rremote: Counting objects:  43% (865/2010)\u001b[K\rremote: Counting objects:  44% (885/2010)\u001b[K\rremote: Counting objects:  45% (905/2010)\u001b[K\rremote: Counting objects:  46% (925/2010)\u001b[K\rremote: Counting objects:  47% (945/2010)\u001b[K\rremote: Counting objects:  48% (965/2010)\u001b[K\rremote: Counting objects:  49% (985/2010)\u001b[K\rremote: Counting objects:  50% (1005/2010)\u001b[K\rremote: Counting objects:  51% (1026/2010)\u001b[K\rremote: Counting objects:  52% (1046/2010)\u001b[K\rremote: Counting objects:  53% (1066/2010)\u001b[K\rremote: Counting objects:  54% (1086/2010)\u001b[K\rremote: Counting objects:  55% (1106/2010)\u001b[K\rremote: Counting objects:  56% (1126/2010)\u001b[K\rremote: Counting objects:  57% (1146/2010)\u001b[K\rremote: Counting objects:  58% (1166/2010)\u001b[K\rremote: Counting objects:  59% (1186/2010)\u001b[K\rremote: Counting objects:  60% (1206/2010)\u001b[K\rremote: Counting objects:  61% (1227/2010)\u001b[K\rremote: Counting objects:  62% (1247/2010)\u001b[K\rremote: Counting objects:  63% (1267/2010)\u001b[K\rremote: Counting objects:  64% (1287/2010)\u001b[K\rremote: Counting objects:  65% (1307/2010)\u001b[K\rremote: Counting objects:  66% (1327/2010)\u001b[K\rremote: Counting objects:  67% (1347/2010)\u001b[K\rremote: Counting objects:  68% (1367/2010)\u001b[K\rremote: Counting objects:  69% (1387/2010)\u001b[K\rremote: Counting objects:  70% (1407/2010)\u001b[K\rremote: Counting objects:  71% (1428/2010)\u001b[K\rremote: Counting objects:  72% (1448/2010)\u001b[K\rremote: Counting objects:  73% (1468/2010)\u001b[K\rremote: Counting objects:  74% (1488/2010)\u001b[K\rremote: Counting objects:  75% (1508/2010)\u001b[K\rremote: Counting objects:  76% (1528/2010)\u001b[K\rremote: Counting objects:  77% (1548/2010)\u001b[K\rremote: Counting objects:  78% (1568/2010)\u001b[K\rremote: Counting objects:  79% (1588/2010)\u001b[K\rremote: Counting objects:  80% (1608/2010)\u001b[K\rremote: Counting objects:  81% (1629/2010)\u001b[K\rremote: Counting objects:  82% (1649/2010)\u001b[K\rremote: Counting objects:  83% (1669/2010)\u001b[K\rremote: Counting objects:  84% (1689/2010)\u001b[K\rremote: Counting objects:  85% (1709/2010)\u001b[K\rremote: Counting objects:  86% (1729/2010)\u001b[K\rremote: Counting objects:  87% (1749/2010)\u001b[K\rremote: Counting objects:  88% (1769/2010)\u001b[K\rremote: Counting objects:  89% (1789/2010)\u001b[K\rremote: Counting objects:  90% (1809/2010)\u001b[K\rremote: Counting objects:  91% (1830/2010)\u001b[K\rremote: Counting objects:  92% (1850/2010)\u001b[K\rremote: Counting objects:  93% (1870/2010)\u001b[K\rremote: Counting objects:  94% (1890/2010)\u001b[K\rremote: Counting objects:  95% (1910/2010)\u001b[K\rremote: Counting objects:  96% (1930/2010)\u001b[K\rremote: Counting objects:  97% (1950/2010)\u001b[K\rremote: Counting objects:  98% (1970/2010)\u001b[K\rremote: Counting objects:  99% (1990/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2009/2009), done.\u001b[K\n",
            "remote: Total 2010 (delta 1), reused 2009 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2010/2010), 3.55 MiB | 16.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at 3 things:\n",
        "\n",
        "1.   Separation of data into training and test sets.\n",
        "2.   Loading and cleaning the data to remove punctuation and numbers.\n",
        "3.   Defining a vocabulary of preferred words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative. This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model.\n",
        "\n",
        "We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the\n",
        "test set that could help us better prepare the data (e.g. the words used) is unavailable during the preparation of data and the training of the model. \n",
        "\n",
        "That being said, we will use the last 100 positive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for testing the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CSD7eIlEul",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Cleaning Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "The text data is already pretty clean, so not much preparation is required. Without getting too much into the details, we will prepare the data using the following method:\n",
        "\n",
        "* Split tokens on white space.\n",
        "* Remove all punctuation from words.\n",
        "* Remove all words that are not purely comprised of alphabetical characters.\n",
        "* Remove all words that are known stop words.\n",
        "* Remove all words that have a length <= 1 character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwd9MUaDoLDt",
        "colab_type": "code",
        "outputId": "a7141906-cae8-496d-e7c8-3378661c7333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqZugLnjz59",
        "colab_type": "code",
        "outputId": "591e036d-6116-4129-e398-08575e30be96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# define base path for dataset\n",
        "base_path = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken'\n",
        "\n",
        "# load one file\n",
        "filename = base_path + '/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# clean doc\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Define a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "It is important to define a vocabulary of known words when using a bag-of-words model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
        "\n",
        "This is dificult to know beforehand and often it is important to test difierent hypotheses about how to construct a useful vocabulary. We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the counter and we can step over all of the reviews in the negative directory and then the positive directory.\n",
        "\n",
        "```python\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhSeu-kGqem3",
        "colab_type": "text"
      },
      "source": [
        "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
        "```python\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c > min_occurane]\n",
        "print(len(tokens))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "Finally, the vocabulary can be saved to a new file called vocab.txt that we can later load and use to filter movie reviews prior to encoding them for modeling.\n",
        "```python\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "outputId": "f8096380-f5aa-4a65-8217-1f02518c4d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9a4PZI02Qs",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to look at extracting features from the reviews ready for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Train CNN With Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will learn a word embedding while training a convolutional neural network on the classification problem. A word embedding is a way of representing text where each word in the vocabulary is represented by a real valued vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space (close in the vector space). This is a more expressive representation for text than more classical methods like bag-of-words, where relationships between words or tokens are ignored, or forced in bigram and trigram approaches.\n",
        "\n",
        "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer. The first step is to load the vocabulary. We will use it to filter out words from movie reviews that\n",
        "we are not interested in.\n",
        "\n",
        "So we have a local file called vocab.txt with one word per line. We can load that file and build a vocabulary as a set for checking the validity of tokens.\n",
        "\n",
        "```python\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "```\n",
        "\n",
        "Next, we need to load all of the training data movie reviews. For that we can adapt the process docs() to load the documents, clean them, and return them as a list of strings, with one document per string. We want each document to be a string for easy encoding as a sequence of integers later. Cleaning the document involves splitting each review based on white space, removing punctuation, and then filtering out all tokens not in the vocabulary.\n",
        "\n",
        "```python\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "```\n",
        "\n",
        "We can call the process docs function for both the neg and pos directories and combine the reviews into a single train or test dataset. We also can define the class labels for the dataset.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab)\n",
        "  pos = process_docs(base_path + '/pos', vocab)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```\n",
        "\n",
        "The next step is to encode each document as a sequence of integers. The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector representation within the embedding. These vectors are random at the beginning of training, but during training become meaningful to the network. We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```\n",
        "\n",
        "Now that the mapping of words to integers has been prepared, we can use it to encode the reviews in the training dataset. We can do that by calling the texts to sequences() function on the Tokenizer. We also need to ensure that all documents have the same length. This is a requirement of Keras for efficient computation. We could truncate reviews to the smallest size\n",
        "or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid.\n",
        "\n",
        "In this case, we will pad all reviews to the length of the longest review in the training dataset. First, we can find the longest review using the max() function on the training dataset and take its length. We can then call the Keras function pad sequences() to pad the sequences to the maximum\n",
        "length by adding 0 values on the end.\n",
        "\n",
        "```python\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "```\n",
        "\n",
        "We can then use the maximum length as a parameter to a function to integer encode and pad the sequences.\n",
        "\n",
        "```python\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "  return padded\n",
        "```\n",
        "\n",
        "We are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer requires the specification of the vocabulary size, the size of the real-valued vector space, and the maximum length of input documents. The vocabulary size is the total number of words in our vocabulary, plus one for unknown words.\n",
        "This could be the vocab set length or the size of the vocab within the tokenizer used to integer encode the documents.\n",
        "\n",
        "```python\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "```\n",
        "\n",
        "We will use a 100-dimensional vector space, but you could try other values, such as 50 or 150. Finally, the maximum document length was calculated above in the max length variable used during padding.\n",
        "\n",
        "We use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems.\n",
        "\n",
        "Next, the 2D output from the CNN part of the model is attened to one long 2D vector to represent the features extracted by the CNN. The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "```\n",
        "\n",
        "Next, we fit the network on the training data.\n",
        "\n",
        "```python\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "```\n",
        "\n",
        "After the model is fit, it is saved to a file named model.h5 for later evaluation.\n",
        "\n",
        "```python\n",
        "# save the model\n",
        "model.save('model.h5')\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "outputId": "f6b0101a-9662-406b-8831-fa4721c11e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  #tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  #stop_words = set(stopwords.words('english'))\n",
        "  #tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [word for word in tokens if word in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "  return padded\n",
        "\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "  model.add(MaxPool1D(pool_size=2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "\n",
        "  # plot model architecture\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size: {str(vocab_size)}')\n",
        "\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print(f'Maximum length: {str(max_length)}')\n",
        "\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, np.array(ytrain), epochs=10, verbose=2)\n",
        "\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1317, 100)         2576800   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 1310, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 655, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20960)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                209610    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1800 samples\n",
            "Epoch 1/10\n",
            "1800/1800 - 18s - loss: 0.6886 - accuracy: 0.5311\n",
            "Epoch 2/10\n",
            "1800/1800 - 17s - loss: 0.5984 - accuracy: 0.6728\n",
            "Epoch 3/10\n",
            "1800/1800 - 17s - loss: 0.1514 - accuracy: 0.9544\n",
            "Epoch 4/10\n",
            "1800/1800 - 17s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1800/1800 - 17s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1800/1800 - 17s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1800/1800 - 17s - loss: 9.0656e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1800/1800 - 17s - loss: 6.1130e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1800/1800 - 17s - loss: 4.6101e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1800/1800 - 17s - loss: 3.6044e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBg8wz25chd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvlF4AgaJBfL",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will evaluate the trained model and use it to make predictions on new data. First, we can use the built-in evaluate() function to estimate the skill of the model on both the training and test dataset.\n",
        "\n",
        "```python\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "```\n",
        "\n",
        "We can then load the model and evaluate it on both datasets and print the accuracy.\n",
        "\n",
        "```python\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "```\n",
        "\n",
        "New data must then be prepared using the same text encoding and encoding schemes as was used on the training dataset. Once prepared, a prediction can be made by calling the predict() function on the model.\n",
        "\n",
        "```python\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "  # clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "  # encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(padded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n",
        "```\n",
        "\n",
        "We can test out this model with two ad hoc movie reviews.The complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfmPTRTpq0Tk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "81b44bd0-c843-462d-c4c1-72f6b0969e93"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [word for word in tokens if word in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "  return padded\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab_size, tokenizer, max_length, model):\n",
        "  # clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "  # encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "  # predict sentiment\n",
        "  y_hat = model.predict(padded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = y_hat[0, 0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1 - percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n",
        "\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size: {str(vocab_size)}')\n",
        "\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print(f'Maximum length: {str(max_length)}')\n",
        "\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, np.array(ytrain), verbose=0)\n",
        "print(f'Train Accuracy: {str(acc * 100)}')\n",
        "\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, np.array(ytest), verbose=0)\n",
        "print(f'Test Accuracy: {str(acc * 100)}')\n",
        "\n",
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print(f'Review: [{text}] \\nSentiment: {sentiment} {str(percent*100)}')\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print(f'Review: [{text}] \\nSentiment: {sentiment} {str(percent*100)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "Train Accuracy: 100.0\n",
            "Test Accuracy: 89.99999761581421\n",
            "Review: [Everyone will enjoy this film. I love it, recommended!] \n",
            "Sentiment: NEGATIVE 51.31205916404724\n",
            "Review: [This is a bad movie. Do not watch it. It sucks.] \n",
            "Sentiment: NEGATIVE 55.50579130649567\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}