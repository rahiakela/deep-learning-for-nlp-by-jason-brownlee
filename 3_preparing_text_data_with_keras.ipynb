{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-preparing-text-data-with-keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMn6G0tHD3H7eSQxpZWOxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-1-data-preparation/3_preparing_text_data_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhqy3bPgpOSE",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Text Data With Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4qc9lFpQbX",
        "colab_type": "text"
      },
      "source": [
        "You cannot feed raw text directly into deep learning models. Text data must be encoded as\n",
        "numbers to be used as input or output for machine learning and deep learning models, such\n",
        "as word embeddings. The Keras deep learning library provides some basic tools to help you\n",
        "prepare your text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoM6uJ6izlsk",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwS8YB7Hzm-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence, one_hot, hashing_trick, Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CTqkrhgpt9A",
        "colab_type": "text"
      },
      "source": [
        "## Split Words with text to word sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbwNuJ0Gpu-7",
        "colab_type": "text"
      },
      "source": [
        "A good first step when working with text is to split it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the\n",
        "text to word sequence() function that you can use to split text into a list of words. \n",
        "By default, this function automatically does 3 things:\n",
        "\n",
        "* Splits words by space.\n",
        "* Filters out punctuation.\n",
        "* Converts text to lowercase (lower=True)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1dYh_IziA0",
        "colab_type": "code",
        "outputId": "5d4de1bf-ee7c-4c03-bdc1-0c9bec8009f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXfnc8Q3Yjr",
        "colab_type": "text"
      },
      "source": [
        "## Encoding with one hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lnQvIQ73bBQ",
        "colab_type": "text"
      },
      "source": [
        "It is popular to represent a document as a sequence of integer values, where each word in the document is represented as a unique integer. Keras provides the one hot() function that you\n",
        "can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one hot encoding of the document, which is not the case. \n",
        "\n",
        "Instead, the function is a wrapper for the hashing trick() function. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values. As with the text to word sequence() function, the one hot() function will make the text lower case, filter out punctuation, and split words based on white space.\n",
        "\n",
        "In addition to the text, the vocabulary size (total words) must be specified. This could be the total number of words in the document or more if you intend to encode additional documents\n",
        "that contains additional words. The size of the vocabulary defines the hashing space from which words are hashed. By default, the hash function is used, alternate hash functions can be specified when calling the hashing trick() function directly.\n",
        "\n",
        "We can use the text to word sequence() function to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_MbFe-U3ULG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "324d5436-e53e-4703-a0c7-e90ebe47f5d0"
      },
      "source": [
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJAyAAH2a-9",
        "colab_type": "text"
      },
      "source": [
        "We can put this together with the one hot() function and encode the words in the document. The vocabulary size is increased by one-third to minimize collisions when hashing words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2zMds682Q5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f3ae23b-0772-4525-92f7-90f0525d6c02"
      },
      "source": [
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size * 1.3))\n",
        "print(result)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 6, 6, 5, 9, 5, 3, 9, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS3Icbyk3AUu",
        "colab_type": "text"
      },
      "source": [
        "## Hash Encoding with hashing trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHShkisl3C_8",
        "colab_type": "text"
      },
      "source": [
        "A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers. An alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n",
        "\n",
        "Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function. It provides more  exibility, allowing you to specify\n",
        "the hash function as either hash (the default) or other hash functions such as the built in md5 function or your own function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJP-1Ba82xAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce21e1f2-6423-4358-f21a-ca7b116e915c"
      },
      "source": [
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size * 1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrjUlJwk4h5N",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVX660Ug4jlQ",
        "colab_type": "text"
      },
      "source": [
        "So far we have looked at one-off convenience methods for preparing text with Keras. Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare\n",
        "multiple text documents. This may be the preferred approach for large projects. Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxxdpKG34UZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define 5 documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',\n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!'\n",
        "]\n",
        "\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK00kHtm5Wdu",
        "colab_type": "text"
      },
      "source": [
        "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
        "* **word_counts**: A dictionary of words and their counts.\n",
        "* **word_docs**: An integer count of the total number of documents that were used to fit the Tokenizer.\n",
        "* **word_index**: A dictionary of words and their uniquely assigned integers.\n",
        "* **document_count**: A dictionary of words and how many documents each appeared in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_umF7Gt5Vkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "74ae7cfc-9d36-4971-cf2e-57a88d6a7b2e"
      },
      "source": [
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.word_docs)\n",
        "print(t.word_index)\n",
        "print(t.document_count)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNWm4js9RgD",
        "colab_type": "text"
      },
      "source": [
        "Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets. The texts to matrix() function on the Tokenizer can be used to\n",
        "create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:\n",
        "\n",
        "* **binary**: Whether or not each word is present in the document. This is the default.\n",
        "* **count**: The count of each word in the document.\n",
        "* **tfidf**: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "* **freq**: The frequency of each word as a ratio of words within each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7_tDvTs55IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0367e297-de49-4c83-a519-6c584cc19dce"
      },
      "source": [
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}