{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-developing-neural-bag-of-words-model-for-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP6yQ81PY27shym0ADm0HAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-2-bag-of-words/2_developing_neural_bag_of_words_model_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Developing a Neural Bag-of-Words Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "Movie reviews can be classified as either favorable or not. The evaluation of movie review text is a classification problem often called sentiment analysis. A popular technique for developing sentiment analysis models is to use a bag-of-words model that transforms documents into vectors where each word in the document is assigned a score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYGI31EsuLHI",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGWSHCUzuMT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x75Yt91gjqtm",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZsk9bujrOP",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh6TKK0tjtWy",
        "colab_type": "code",
        "outputId": "698276f6-a864-4824-aea9-ce924be9b5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'machine-learning-datasets' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at 3 things:\n",
        "\n",
        "1.   Separation of data into training and test sets.\n",
        "2.   Loading and cleaning the data to remove punctuation and numbers.\n",
        "3.   Defining a vocabulary of preferred words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative. This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model.\n",
        "\n",
        "We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the\n",
        "test set that could help us better prepare the data (e.g. the words used) is unavailable during the preparation of data and the training of the model. \n",
        "\n",
        "That being said, we will use the last 100 positive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for testing the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CSD7eIlEul",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Cleaning Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "The text data is already pretty clean, so not much preparation is required. Without getting too much into the details, we will prepare the data using the following method:\n",
        "\n",
        "* Split tokens on white space.\n",
        "* Remove all punctuation from words.\n",
        "* Remove all words that are not purely comprised of alphabetical characters.\n",
        "* Remove all words that are known stop words.\n",
        "* Remove all words that have a length <= 1 character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwd9MUaDoLDt",
        "colab_type": "code",
        "outputId": "814008b9-b590-475f-ce08-3b5e1e028b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqZugLnjz59",
        "colab_type": "code",
        "outputId": "624e0058-6a52-4810-b083-931c4757b6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# define base path for dataset\n",
        "base_path = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken'\n",
        "\n",
        "# load one file\n",
        "filename = base_path + '/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# clean doc\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Define a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "It is important to define a vocabulary of known words when using a bag-of-words model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
        "\n",
        "This is dificult to know beforehand and often it is important to test difierent hypotheses about how to construct a useful vocabulary. We have already seen how we can remove punctuation and numbers from the vocabulary in the previous section. We can repeat this for all documents and build a set of all known words.\n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the counter and we can step over all of the reviews in the negative directory and then the positive directory.\n",
        "\n",
        "```python\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhSeu-kGqem3",
        "colab_type": "text"
      },
      "source": [
        "We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
        "```python\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c > min_occurane]\n",
        "print(len(tokens))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "Finally, the vocabulary can be saved to a new file called vocab.txt that we can later load and use to filter movie reviews prior to encoding them for modeling.\n",
        "```python\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f6a3fea7-6856-488e-eb7a-c2a0b2c3e98d"
      },
      "source": [
        "from os import listdir\n",
        "from collections import Counter\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs(base_path + '/pos', vocab)\n",
        "process_docs(base_path + '/neg', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with > 2 occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k, c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "25767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Bag-of-Words Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at how we can convert each review into a representation that we can provide to a Multilayer Perceptron model. A bag-of-words model is a way of extracting\n",
        "features from text so the text input can be used with machine learning algorithms like neural networks. Each document, in this case a review, is converted into a vector representation.\n",
        "\n",
        "The number of items in the vector representing a document corresponds to the number of words in the vocabulary. The larger the vocabulary, the longer the vector representation, hence\n",
        "the preference for smaller vocabularies in the previous section.\n",
        "\n",
        "Words in a document are scored and the scores are placed in the corresponding location in the representation. We will look at different word scoring methods in the next section. In this\n",
        "section, we are concerned with converting reviews into vectors ready for training a first neural network model. \n",
        "\n",
        "This section is divided into 2 steps:\n",
        "* Converting reviews to lines of tokens.\n",
        "* Encoding reviews with a bag-of-words model representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-7KDUKmgyc4",
        "colab_type": "text"
      },
      "source": [
        "### Reviews to Lines of Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHp7MeSAgzlJ",
        "colab_type": "text"
      },
      "source": [
        "Before we can convert reviews to vectors for modeling, we must first clean them up. This involves loading them, performing the cleaning operation developed above, filtering out words\n",
        "not in the chosen vocabulary, and converting the remaining tokens into a single string or line ready for encoding.\n",
        "\n",
        "```python\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "```\n",
        "\n",
        "We can call the process docs() consistently for positive and negative reviews to construct a dataset of review text and their associated output labels, 0 for negative and 1 for positive.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab)\n",
        "  pos = process_docs(base_path + '/pos', vocab)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```\n",
        "\n",
        "Finally, we need to load the vocabulary and turn it into a set for use in cleaning reviews.\n",
        "\n",
        "```python\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "len(vocab)\n",
        "\n",
        "# load all training reviews\n",
        "docs, labels = load_clean_dataset(vocab)\n",
        "# summarize what we have\n",
        "print(len(docs), len(labels))\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHgm9Hqukqlg",
        "colab_type": "text"
      },
      "source": [
        "### Movie Reviews to Bag-of-Words Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znsuEf27krr8",
        "colab_type": "text"
      },
      "source": [
        "We will use the Keras API to convert reviews to encoded document vectors. Keras provides the Tokenizer class that can do some of the cleaning and vocab definition tasks.It is better to do this ourselves to know exactly what was done and why. \n",
        "\n",
        "Nevertheless, the Tokenizer class is convenient and will easily transform documents into encoded vectors. First, the Tokenizer must be created, then fit on the text documents in the training dataset. In this case, these are the aggregation of the positive lines and negative lines arrays.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```\n",
        "\n",
        "This process determines a consistent way to convert the vocabulary to a fixed-length vector with 25,768 elements, which is the total number of words in the vocabulary file vocab.txt.\n",
        "\n",
        "Next, documents can then be encoded using the Tokenizer by calling texts to matrix(). The function takes both a list of documents to encode and an encoding mode, which is the method\n",
        "used to score words in the document. Here we specify freq to score words based on their frequency in the document. This can be used to encode the loaded training and test data, for example.\n",
        "\n",
        "```python\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "```\n",
        "This encodes all of the positive and negative reviews in the training dataset.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "```\n",
        "\n",
        "Similarly, the load clean dataset() dataset must be updated to load either train or test data.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "outputId": "d75cd716-5953-4172-c6b5-aeba24c859ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', vocab, is_train)\n",
        "  pos = process_docs(base_path + '/pos', vocab, is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "print(len(train_docs))\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1800\n",
            "(1800, 25768) (200, 25768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkeDOIHF5WRO",
        "colab_type": "text"
      },
      "source": [
        "The shape of the encoded training dataset and test dataset\n",
        "with 1,800 and 200 documents respectively, each with the same sized encoding vocabulary (vector length)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBg8wz25chd",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARD7nUYgtrR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}