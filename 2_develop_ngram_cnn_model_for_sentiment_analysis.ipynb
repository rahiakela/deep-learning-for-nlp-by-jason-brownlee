{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_develop_ngram_cnn_model_for_sentiment_analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-4-text-classi-cation/2_develop_ngram_cnn_model_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop an n-gram CNN Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "A standard deep learning model for text classification and sentiment analysis uses a word embedding layer and one-dimensional convolutional neural network. The model can be expanded by using multiple parallel convolutional neural networks that read the source document using\n",
        "different kernel sizes. This, in effect, creates a multichannel convolutional neural network for text that reads text with different n-gram sizes (groups of words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "outputId": "d8f0851b-6c29-4656-8f6c-db341d77fbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Conv1D, Dropout, MaxPool1D, Concatenate\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x75Yt91gjqtm",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZsk9bujrOP",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh6TKK0tjtWy",
        "colab_type": "code",
        "outputId": "6c289eaf-3d1e-4d6d-a63e-7aed4d2ae1bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'machine-learning-datasets' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at 3 things:\n",
        "\n",
        "1.   Separation of data into training and test sets.\n",
        "2.   Loading and cleaning the data to remove punctuation and numbers.\n",
        "3.   Defining a vocabulary of preferred words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative. This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model.\n",
        "\n",
        "We will ensure that this constraint is built into the evaluation of our models by splitting the training and test datasets prior to any data preparation. This means that any knowledge in the\n",
        "test set that could help us better prepare the data (e.g. the words used) is unavailable during the preparation of data and the training of the model. \n",
        "\n",
        "That being said, we will use the last 100 positive reviews and the last 100 negative reviews as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. The split can be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for testing the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CSD7eIlEul",
        "colab_type": "text"
      },
      "source": [
        "### Loading and Cleaning Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "The text data is already pretty clean, so not much preparation is required. Without getting too much into the details, we will prepare the data using the following method:\n",
        "\n",
        "* Split tokens on white space.\n",
        "* Remove all punctuation from words.\n",
        "* Remove all words that are not purely comprised of alphabetical characters.\n",
        "* Remove all words that are known stop words.\n",
        "* Remove all words that have a length <= 1 character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwd9MUaDoLDt",
        "colab_type": "code",
        "outputId": "ef1d8fe8-40f1-4759-859d-1c5e97ea5539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxqZugLnjz59",
        "colab_type": "code",
        "outputId": "0794e4e8-60dd-4e76-d20d-84800038d8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# define base path for dataset\n",
        "base_path = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken'\n",
        "\n",
        "# load one file\n",
        "filename = base_path + '/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# clean doc\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Clean All Reviews and Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "We can now use the function to clean reviews and apply it to all reviews. To do this, we will develop a new function named process docs() below that will walk through all reviews in a directory, clean them, and return them as a list. We will also add an argument to the function to indicate whether the function is processing train or test reviews, that way the filenames can\n",
        "be filtered and only those train or test reviews requested will be cleaned\n",
        "and returned.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "```\n",
        "\n",
        "We can call this function with negative training reviews. We also need labels for the train and test documents. We know that we have 900 training documents and 100 test documents. We can use a Python list comprehension to create the labels for the negative (0) and positive (1) reviews for both train and test sets.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "  # load documents\n",
        "  neg = process_docs('txt_sentoken/neg', is_train)\n",
        "  pos = process_docs('txt_sentoken/pos', is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "Finally, we want to save the prepared train and test sets to file so that we can load them later for modeling and model evaluation.\n",
        "\n",
        "```python\n",
        "def save_dataset(lines, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "outputId": "ac65a5e8-6407-48c7-f67a-61714cd68d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  tokens = ' '.join(tokens)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue \n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # add to list\n",
        "    documents.append(documents)\n",
        "\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', is_train)\n",
        "  pos = process_docs(base_path + '/pos', is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\n",
        "  return docs, labels\n",
        "\n",
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print(f'Saved: {filename}')\n",
        "\n",
        "# load and clean all reviews\n",
        "train_docs, ytrain = load_clean_dataset(True)\n",
        "test_docs, ytest = load_clean_dataset(False)\n",
        "\n",
        "# save training datasets\n",
        "save_dataset([train_docs, ytrain], 'train.pkl')\n",
        "save_dataset([test_docs, ytest], 'test.pkl')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9a4PZI02Qs",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to develop our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Develop Multichannel Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5a8rWe353s",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will develop a multichannel convolutional neural network for the sentiment analysis prediction problem. This section is divided into 3 parts:\n",
        "\n",
        "1.  Encode Data\n",
        "2.  Define Model.\n",
        "3.  Complete Example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ifwli636pl",
        "colab_type": "text"
      },
      "source": [
        "### Encode Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "The first step is to load the cleaned training dataset. The function below-named load dataset() can be called to load the pickled training dataset.\n",
        "\n",
        "\n",
        "```python\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "  return load(open(filename, 'rb'))\n",
        "\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "```\n",
        "\n",
        "Next, we must fit a Keras Tokenizer on the training dataset. We will use this tokenizer to both define the vocabulary for the Embedding layer and encode the review documents as integers.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```\n",
        "\n",
        "We also need to know the maximum length of input sequences as input for the model and to pad all sequences to the fixed length.\n",
        "\n",
        "The function max length() below will calculate the maximum length (number of words) for all reviews in the training dataset.\n",
        "\n",
        "```python\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "  return max([len(s.split()) for s in lines])\n",
        "```\n",
        "\n",
        "We also need to know the size of the vocabulary for the Embedding layer. This can be calculated from the prepared Tokenizer.\n",
        "\n",
        "```python\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "```\n",
        "\n",
        "Finally, we can integer encode and pad the clean movie review text. The function below named encode text() will both encode and pad text data to the maximum review length.\n",
        "\n",
        "```python\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad encoded sequences\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOSY4i76vFF",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jCLBKNh6xl9",
        "colab_type": "text"
      },
      "source": [
        "A standard model for document classification is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. The kernel size in the convolutional layer defines the number of words to consider as the convolution is passed across the input text document, providing a grouping parameter. \n",
        "\n",
        "A multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels. This allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n",
        "\n",
        "In Keras, a multiple-input model can be defined using the functional API. We will define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. Each channel is comprised of the following elements:\n",
        "\n",
        "* **Input layer** that defines the length of input sequences.\n",
        "* **Embedding layer** set to the size of the vocabulary and 100-dimensional real-valued representations.\n",
        "* **Conv1D layer** with 32 filters and a kernel size set to the number of words to read at once.\n",
        "* **MaxPooling1D layer** to consolidate the output from the convolutional layer.\n",
        "* **Flatten layer** to reduce the three-dimensional output to two dimensional for concatenation.\n",
        "\n",
        "The output from the three channels are concatenated into a single vector and process by a Dense layer and an output layer.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "\n",
        "  # channel 1\n",
        "  inputs1 = Input(shape=(length,))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  # channel 2\n",
        "  inputs2 = Input(shape=(length,))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  # channel 3\n",
        "  inputs3 = Input(shape=(length,))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  # interpretation\n",
        "  dense1 = Dense(10, activation='relu')(merged)\n",
        "  outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  # summarize\n",
        "  model.summary()\n",
        "  plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import load\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def load_dataset(filename):\n",
        "  # load dataset\n",
        "  return load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "  return max([len(line.split()) for line in lines])\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, max_length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "  return padded\n",
        "\n",
        "# define the model\n",
        "def define_model(max_length, vocab_size):\n",
        "\n",
        "  print('Creating channel.......')\n",
        "  # channel 1\n",
        "  inputs1 = Input(shape=(max_length, ))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPool1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  print('Creating channe2.......')\n",
        "  # channel 2\n",
        "  inputs2 = Input(shape=(max_length, ))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPool1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  print('Creating channe3.......')\n",
        "  # channel 3\n",
        "  inputs3 = Input(shape=(max_length, ))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPool1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  print('Creating all channes.......')\n",
        "  # merge all channel\n",
        "  merged_layer = Concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  # interpretation\n",
        "  dense_layer = Dense(10, activation='relu')(merged_layer)\n",
        "  output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "\n",
        "  print('Creating model.......')\n",
        "  # create model\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output_layer)\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "\n",
        "  # plot model architecture\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Loadin dataset.......')\n",
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "# convert to array\n",
        "trainLines = np.array(trainLines)\n",
        "trainLabels = np.array(trainLabels)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "\n",
        "# calculate the maximum document length\n",
        "max_length = max_length(trainLines)\n",
        "print(f'Maximum document length: {str(max_length)}')\n",
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size: {str(vocab_size)}')\n",
        "\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, max_length)\n",
        "\n",
        "print('Creating model.......')\n",
        "# define model\n",
        "model = define_model(max_length, vocab_size)\n",
        "\n",
        "print('Traing model.......')\n",
        "# fit model\n",
        "model.fit([trainX, trainX, trainX], trainLabels, epochs=7, batch_size=16, verbose=1)\n",
        "\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBg8wz25chd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    }
  ]
}