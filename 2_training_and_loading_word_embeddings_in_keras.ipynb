{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-training-and-loading-word-embeddings-in-keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxpdPQFPywvzgbabIZQ2U+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-3-word-embeddings/2_training_and_loading_word_embeddings_in_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPs9mT0jPnzS",
        "colab_type": "text"
      },
      "source": [
        "# Training and Loading Word Embeddings in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TG71b49P25r",
        "colab_type": "text"
      },
      "source": [
        "Word embeddings provide a dense representation of words and their relative meanings. They are an improvement over sparse representations used in simpler bag of word model representations.\n",
        "\n",
        "Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ICDr_1SQF3x",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zvC4e_MQIHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08fd43c9-752e-4d46-ae9c-7a7c1be9c2cc"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwnLlboQIsj",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmRzYbvwQJon",
        "colab_type": "text"
      },
      "source": [
        "A word embedding is a class of approaches for representing words and documents using a dense vector representation. It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. \n",
        "\n",
        "These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
        "\n",
        "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding. \n",
        "\n",
        "Two popular examples of methods of learning word embeddings from text include:\n",
        "* **Word2Vec**\n",
        "* **GloVe**\n",
        "\n",
        "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific\n",
        "training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcTe5gGbLCL",
        "colab_type": "text"
      },
      "source": [
        "## Train word-embedding using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4501Yq8SsVZ",
        "colab_type": "text"
      },
      "source": [
        "### Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBqfVjAQStFe",
        "colab_type": "text"
      },
      "source": [
        "Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer.\n",
        "\n",
        "This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
        "\n",
        "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. It is a  exible layer that can be used in a variety of ways, such as:\n",
        "\n",
        "* It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
        "* It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "* It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
        "\n",
        "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
        "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could\n",
        "be 32 or 100 or even larger. Test different values for your problem.\n",
        "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000\n",
        "words, this would be 1000.\n",
        "\n",
        "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
        "\n",
        "```python\n",
        "e = Embedding(200, 32, input_length=50)\n",
        "```\n",
        "\n",
        "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer. The output of the Embedding layer is a 2D vector with\n",
        "one embedding for each word in the input sequence of words (input document). If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output\n",
        "matrix to a 1D vector using the Flatten layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPJu9JARU1kv",
        "colab_type": "text"
      },
      "source": [
        "### Example of Learning an Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pr2trStU2j3",
        "colab_type": "text"
      },
      "source": [
        "We will look at how we can learn a word embedding while fitting a neural network on a text classification problem. We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classiffied as positive 1 or negative 0. This is a simple sentiment analysis problem.\n",
        "\n",
        "```python\n",
        "# define documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',\n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!',\n",
        "  'Weak',\n",
        "  'Poor effort!',\n",
        "  'not good',\n",
        "  'poor work',\n",
        "  'Could have done better.'\n",
        "]\n",
        "# define class labels\n",
        "labels = [1,1,1,1,1,0,0,0,0,0]\n",
        "```\n",
        "\n",
        "Next, we can integer encode each document. This means that as input the Embedding layer will have sequences of integers. We could experiment with other more sophisticated bag of word\n",
        "model encoding like counts or TF-IDF. Keras provides the one hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 50,\n",
        "which is much larger than needed to reduce the probability of collisions from the hash function.\n",
        "\n",
        "```python\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "```\n",
        "\n",
        "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4. Again, we can\n",
        "do this with a built in Keras function, in this case the pad sequences() function.\n",
        "\n",
        "```python\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "```\n",
        "\n",
        "We are now ready to define our Embedding layer as part of our neural network model. The Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions. The model is a simple binary classification model.\n",
        "\n",
        "Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "Finally, we can fit and evaluate the classi\fcation model.\n",
        "\n",
        "```python\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "```\n",
        "\n",
        "Let's put it all together.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxfLACHXW6QM",
        "colab_type": "code",
        "outputId": "da881d30-ac8e-4bdf-866e-b3d7aabf9230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# define documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',   \n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!',\n",
        "  'Weak',\n",
        "  'Poor effort!',\n",
        "  'not good',\n",
        "  'poor work',\n",
        "  'Could have done better.'   \n",
        "]\n",
        "\n",
        "# define class labels\n",
        "labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
        "\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(doc, vocab_size) for doc in docs]\n",
        "print(f'Encoded docs: \\n{encoded_docs}')\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(f'Padded docs: \\n{padded_docs}')\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print('Model Summary:\\n')\n",
        "model.summary()\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print(f'Accuracy: {str(accuracy * 100)}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded docs: \n",
            "[[26, 15], [18, 15], [32, 29], [42, 15], [1], [12], [19, 29], [40, 18], [19, 15], [35, 39, 15, 48]]\n",
            "Padded docs: \n",
            "[[26 15  0  0]\n",
            " [18 15  0  0]\n",
            " [32 29  0  0]\n",
            " [42 15  0  0]\n",
            " [ 1  0  0  0]\n",
            " [12  0  0  0]\n",
            " [19 29  0  0]\n",
            " [40 18  0  0]\n",
            " [19 15  0  0]\n",
            " [35 39 15 48]]\n",
            "Model Summary:\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 89.99999761581421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6sT3ejca8TK",
        "colab_type": "text"
      },
      "source": [
        "You could save the learned weights from the Embedding layer to file for later use in other models. You could also use this model generally to classify other documents that have the same kind vocabulary seen in the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG-N8mtqbDqL",
        "colab_type": "text"
      },
      "source": [
        "## Using Pre-Trained GloVe Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFsRIlxbEsr",
        "colab_type": "text"
      },
      "source": [
        "The Keras Embedding layer can also use a word embedding learned elsewhere. It is common in the field of Natural Language Processing to learn, save, and make freely available word embeddings.\n",
        "\n",
        "You can download GloVe embeddings and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your training dataset.\n",
        "\n",
        "As in the previous section, the first step is to define the examples, encode them as integers, then pad the sequences to be the same length. In this case, we need to be able to map words to\n",
        "integers as well as integers to words. \n",
        "\n",
        "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts to sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word index attribute.\n",
        "\n",
        "```python\n",
        "# define documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',\n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!',\n",
        "  'Weak',\n",
        "  'Poor effort!',\n",
        "  'not good',\n",
        "  'poor work',\n",
        "  'Could have done better.'\n",
        "]\n",
        "\n",
        "# define class labels\n",
        "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "```\n",
        "\n",
        "Next, we need to load the entire GloVe word embedding file into memory as a dictionary of word to embedding array.\n",
        "\n",
        "```python\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "```\n",
        "\n",
        "This is pretty slow. It might be better to filter the embedding for the unique words in your training data. \n",
        "\n",
        "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding. The result is a matrix of weights only for words we will see during training.\n",
        "\n",
        "```python\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "```\n",
        "\n",
        "Now we can define our model, fit, and evaluate it as before. The key difference is that the Embedding layer can be seeded with the GloVe word embedding weights. We chose the 100-dimensional version, therefore the Embedding layer must be defined with output dim set to 100. \n",
        "\n",
        "Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False.\n",
        "\n",
        "```python\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "```\n",
        "\n",
        "Let's put it all together.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QbWjw0_6UIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import requests\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4QZLyDR6Y2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pugnlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_kKxVfp6cn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pugnlp.futil import path_status, find_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmnmI7aP6gRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIG_URLS = {\n",
        "    'w2v': ('https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1', 1647046227),\n",
        "    'g2v': ('http://nlp.stanford.edu/data/glove.6B.zip',)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cax3qVMe6i3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
        "def dropbox_basename(url):\n",
        "    filename = os.path.basename(url)\n",
        "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
        "    if match:\n",
        "        return filename[:-len(match[0])]\n",
        "    return filename\n",
        "\n",
        "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
        "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
        "    if filename is None:\n",
        "        filename = dropbox_basename(url)\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    if url.endswith('?dl=0'):\n",
        "        url = url[:-1] + '1'  # noninteractive download\n",
        "    if verbose:\n",
        "        tqdm_prog = tqdm\n",
        "        print('requesting URL: {}'.format(url))\n",
        "    else:\n",
        "        tqdm_prog = no_tqdm\n",
        "    r = requests.get(url, stream=True, allow_redirects=True)\n",
        "    size = r.headers.get('Content-Length', None) if size is None else size\n",
        "    print('remote size: {}'.format(size))\n",
        "\n",
        "    stat = path_status(file_path)\n",
        "    print('local size: {}'.format(stat.get('size', None)))\n",
        "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
        "        r.close()\n",
        "        return file_path\n",
        "\n",
        "    print('Downloading to {}'.format(file_path))\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "    r.close()\n",
        "    return file_path\n",
        "\n",
        "def untar(fname):\n",
        "    if fname.endswith(\".gz\"):\n",
        "        with tarfile.open(fname) as tf:\n",
        "            tf.extractall()\n",
        "    else:\n",
        "        print(\"Not a tar.gz file: {}\".format(fname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEZpUDsX6mqg",
        "colab_type": "code",
        "outputId": "edf5e300-8083-498c-b9ef-012efc097aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "download_file(BIG_URLS['g2v'][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: http://nlp.stanford.edu/data/glove.6B.zip\n",
            "remote size: 862182613\n",
            "local size: None\n",
            "Downloading to ./glove.6B.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./glove.6B.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqvmttNS64sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip zip file\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKnTReajZlPF",
        "colab_type": "code",
        "outputId": "35227970-68bb-427b-9918-287ca9ef30d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# define documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',   \n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!',\n",
        "  'Weak',\n",
        "  'Poor effort!',\n",
        "  'not good',\n",
        "  'poor work',\n",
        "  'Could have done better.'   \n",
        "]\n",
        "\n",
        "# define class labels\n",
        "labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(f'Encoded docs: \\n{encoded_docs}')\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(f'Padded docs: \\n{padded_docs}')\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "file = open('glove/glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "file.close()\n",
        "print(f'Loaded {str(len(embeddings_index))} word vectors.')\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  \n",
        "# define model\n",
        "model = Sequential()\n",
        "embedding = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "model.add(embedding)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "model.summary()\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print(f'Accuracy: \\n{str(accuracy * 100)}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded docs: \n",
            "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
            "Padded docs: \n",
            "[[ 6  2  0  0]\n",
            " [ 3  1  0  0]\n",
            " [ 7  4  0  0]\n",
            " [ 8  1  0  0]\n",
            " [ 9  0  0  0]\n",
            " [10  0  0  0]\n",
            " [ 5  4  0  0]\n",
            " [11  3  0  0]\n",
            " [ 5  1  0  0]\n",
            " [12 13  2 14]]\n",
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 4, 100)            1500      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 1,901\n",
            "Trainable params: 401\n",
            "Non-trainable params: 1,500\n",
            "_________________________________________________________________\n",
            "Accuracy: \n",
            "100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqhv76C2CoI6",
        "colab_type": "text"
      },
      "source": [
        "## Using Pre-Trained Word2Vec Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrgfsL_XCsDX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDu_fzmA_u8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2ee84ab9-c17f-4203-e551-08b4d1eebb3c"
      },
      "source": [
        "# download Word2Vec embedding\n",
        "download_file(BIG_URLS['w2v'][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "remote size: 1647046227\n",
            "local size: None\n",
            "Downloading to ./GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./GoogleNews-vectors-negative300.bin.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPamT28QFlHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip gzip file\n",
        "import gzip\n",
        "import shutil\n",
        "with gzip.open('GoogleNews-vectors-negative300.bin.gz', 'rb') as f_in:\n",
        "    with open('embedding_word2vec.txt', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU5-v1qHJi1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory, skip first line\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\t# create a map of words to vectors\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        " \n",
        "# create a weight matrix for the Embedding layer from a loaded embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = np.zeros((vocab_size, 100))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tweight_matrix[i] = embedding.get(word)\n",
        "\treturn weight_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfFeFwqHHgsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define documents\n",
        "docs = [\n",
        "  'Well done!',\n",
        "  'Good work',   \n",
        "  'Great effort',\n",
        "  'nice work',\n",
        "  'Excellent!',\n",
        "  'Weak',\n",
        "  'Poor effort!',\n",
        "  'not good',\n",
        "  'poor work',\n",
        "  'Could have done better.'   \n",
        "]\n",
        "\n",
        "# define class labels\n",
        "labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(f'Encoded docs: \\n{encoded_docs}')\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(f'Padded docs: \\n{padded_docs}')\n",
        "\n",
        "# load the whole embedding into memory\n",
        "# load embedding from file\n",
        "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
        "# get vectors in the right order\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "  \n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "model.summary()\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print(f'Accuracy: \\n{str(accuracy * 100)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnqRdLydJRxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}