{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-clean-text-manually-and-with-nltk.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/F+CEP0ZFMO4PPXquZbbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-1-data-preparation/1_clean_text_manually_and_with_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHnyaU0cRExg",
        "colab_type": "text"
      },
      "source": [
        "# Clean Text Manually and with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-J6KHN6RJDg",
        "colab_type": "text"
      },
      "source": [
        "You cannot go straight from raw text to fitting a machine learning or deep learning model. You\n",
        "must clean your text first, which means splitting it into words and handling punctuation and\n",
        "case. In fact, there is a whole suite of text preparation methods that you may need to use, and\n",
        "the choice of methods really depends on your natural language processing task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUb0HFWrRb7R",
        "colab_type": "text"
      },
      "source": [
        "## Manual Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk-vVRnJRdFK",
        "colab_type": "text"
      },
      "source": [
        "Text cleaning is hard, but the text we have chosen to work with is pretty clean already. We\n",
        "could just write some Python code to clean it up manually, and this is a good exercise for those\n",
        "simple problems that you encounter. Tools like regular expressions and splitting strings can get\n",
        "you a long way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0JYxf-ZSBVW",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF2R5jMaSCfV",
        "colab_type": "text"
      },
      "source": [
        "Let's load the text data so that we can work with it. The text is small and will load quickly\n",
        "and easily fit into memory. This will not always be the case and you may need to write code\n",
        "to memory map the file. Tools like NLTK (covered in the next section) will make working\n",
        "with large files much easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAtTh73uSOve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename ='metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObK2DJspTVuK",
        "colab_type": "text"
      },
      "source": [
        "### Split by Whitespace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOtwxPsATW2x",
        "colab_type": "text"
      },
      "source": [
        "Clean text often means a list of words or tokens that we can work with in our machine learning\n",
        "models. This means converting the raw text into a list of words and saving it again. A very\n",
        "simple way to do this would be to split the document by white space, including \\ \" (space), new\n",
        "lines, tabs and more. We can do this in Python with the split() function on the loaded string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0yFBeqXTP5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b81f51b7-aee5-40da-de71-f54072d1280f"
      },
      "source": [
        "# split into words by white space\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffOne', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t34w2BvZUA0W",
        "colab_type": "text"
      },
      "source": [
        "### Select Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DvqeVSgUB2q",
        "colab_type": "text"
      },
      "source": [
        "Another approach might be to use the regex model (re) and split the document into words by\n",
        "selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ` ')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTlRN9nMTz1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "15420f5c-2d1d-4e17-d5c3-82588f55a6e3"
      },
      "source": [
        "import re\n",
        "\n",
        "# split based on words only\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:100])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tXqHN3QUnUa",
        "colab_type": "text"
      },
      "source": [
        "### Split by Whitespace and Remove Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgHgFhukUoWa",
        "colab_type": "text"
      },
      "source": [
        "We may want the words, but without the punctuation like commas and quotes. We also want to\n",
        "keep contractions together. One way would be to split the document into words by white space then use string translation to replace all punctuation with\n",
        "nothing (e.g. remove it). Python provides a constant called string.punctuation that provides a\n",
        "great list of punctuation characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7FpuU7yUach",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7816feae-2a0a-4713-f9e0-7f337891df3e"
      },
      "source": [
        "import string \n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSbwLGzKWIfq",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l56AAZeWJki",
        "colab_type": "text"
      },
      "source": [
        "It is common to convert all words to one case. This means that the vocabulary will shrink in\n",
        "size, but some distinctions are lost (e.g. Apple the company vs apple the fruit is a commonly\n",
        "used example). We can convert all words to lowercase by calling the lower() function on each\n",
        "word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RKnFeWGU9JD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8c0bd5b2-525c-4454-92fe-9de564c87212"
      },
      "source": [
        "# convert to lower case\n",
        "words = [word.lower() for word in stripped]\n",
        "print(words[:100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'what', 's', 'happened', 'to', 'me', 'he', 'thought', 'it', 'wasn', 't', 'a', 'dream', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cQVQLExWyoR",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Cleaning text is really hard, problem specific, and full of trade-offs. Remember, simple is better.\n",
        "Simpler text data, simpler models, smaller vocabularies. You can always make things more complex later to see if it results in better model skill."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XejjjYn2XQhK",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Cleaning with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYRWYRJPXRmt",
        "colab_type": "text"
      },
      "source": [
        "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text. It provides good tools for loading and cleaning text that we can use to get our data ready for working with machine learning and deep learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POcPHUXnX-85",
        "colab_type": "text"
      },
      "source": [
        "### Split into Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUPQV1pYAvW",
        "colab_type": "text"
      },
      "source": [
        "A good useful first step is to split the text into sentences. Some modeling tasks prefer input\n",
        "to be in the form of paragraphs or sentences, such as Word2Vec. You could first split your\n",
        "text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
        "NLTK provides the sent tokenize() function to split text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-KRzkM7YfuP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "31be0c25-7f4c-4ac7-b6a5-0c2e2c80663a"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdcr0RouX1qg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "56836aca-5c53-42a0-9b12-f135aea74d43"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "# split into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX7jqh1SYnU5",
        "colab_type": "text"
      },
      "source": [
        "### Split into Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_BlbKEZYqFg",
        "colab_type": "text"
      },
      "source": [
        "NLTK provides a function called word tokenize() for splitting strings into tokens (nominally\n",
        "words). It splits tokens based on white space and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkPAtgWKYZI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1c16dc58-190e-49ef-d21c-bca4f71f66d6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffOne', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCFkDnZ5ZKF8",
        "colab_type": "text"
      },
      "source": [
        "### Filter Out Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1XVX7rDZLA6",
        "colab_type": "text"
      },
      "source": [
        "We can filter out all tokens that we are not interested in, such as all standalone punctuation. This\n",
        "can be done by iterating over all tokens and only keeping those tokens that are all alphabetic.\n",
        "Python has the function isalpha() that can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qePBv5O5ZCDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3e144c42-cd24-4645-f139-75a5d2e0f4d4"
      },
      "source": [
        "# remove all tokens that are not alphabetic\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words[:100])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTtz2STmZ1Vt",
        "colab_type": "text"
      },
      "source": [
        "### Filter out Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfo7r02zZ2LA",
        "colab_type": "text"
      },
      "source": [
        "Stop words are those words that do not contribute to the deeper meaning of the phrase. They\n",
        "are the most common words such as: the, a, and is. For some applications like documentation\n",
        "classification, it may make sense to remove stop words. NLTK provides a list of commonly\n",
        "agreed upon stop words for a variety of languages, such as English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdPmrKzuaSCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "758d9343-df92-4373-9abf-85f304098f79"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J4HFodyZvFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "23d2f4b8-275b-4f6f-c13a-33550ae8cc71"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaHX8JKkah4e",
        "colab_type": "text"
      },
      "source": [
        "### Text cleaning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zU5n--4alCi",
        "colab_type": "text"
      },
      "source": [
        "Let's demonstrate this with a small pipeline of text preparation including:\n",
        "\n",
        "* Load the raw text.\n",
        "* Split into tokens.\n",
        "* Convert to lowercase.\n",
        "* Remove punctuation from each token.\n",
        "* Filter out remaining tokens that are not alphabetic.\n",
        "* Filter out tokens that are stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT_y3Fk-aPK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8893e749-dc5a-40bf-ff63-8c4983f4204c"
      },
      "source": [
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'gregor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjua2oJRcEyv",
        "colab_type": "text"
      },
      "source": [
        "### Stem Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAdlH5MBcFx5",
        "colab_type": "text"
      },
      "source": [
        "Stemming refers to the process of reducing each word to its root or base. For example fishing,\n",
        "fished, fisher all reduce to the stem fish. \n",
        "\n",
        "Some applications, like document classification, may\n",
        "benefit from stemming in order to both reduce the vocabulary and to focus on the sense or\n",
        "sentiment of a document rather than deeper meaning. There are many stemming algorithms,\n",
        "although a popular and long-standing method is the Porter Stemming algorithm. This method\n",
        "is available in NLTK via the PorterStemmer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgo_YNMWb_Fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7a943ca1-3852-442d-c543-12e78e691cc2"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# stemming of words\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:100])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffone', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'he', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS5zoHz-dAdY",
        "colab_type": "text"
      },
      "source": [
        "There is a nice suite of stemming and lemmatization algorithms to choose from in NLTK, if\n",
        "reducing words to their root is something you need for your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag3ZKoOedBNO",
        "colab_type": "text"
      },
      "source": [
        "## Additional Text Cleaning Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpU24gBidEMS",
        "colab_type": "text"
      },
      "source": [
        "We are only getting started. Because the source text for this tutorial was reasonably clean to\n",
        "begin with, we skipped many concerns of text cleaning that you may need to deal with in your\n",
        "own project. Here is a shortlist of additional considerations when cleaning text:\n",
        "\n",
        "* Handling large documents and large collections of text documents that do not fit into\n",
        "memory.\n",
        "* Extracting text from markup like HTML, PDF, or other structured document formats.\n",
        "* Transliteration of characters from other languages into English.\n",
        "* Decoding Unicode characters into a normalized form, such as UTF8.\n",
        "* Handling of domain specific words, phrases, and acronyms.\n",
        "* Handling or removing numbers, such as dates and amounts.\n",
        "* Locating and correcting common typos and misspellings.\n",
        "* And much more...\n",
        "\n",
        "The list could go on. Hopefully, you can see that getting truly clean text is impossible, that\n",
        "we are really doing the best we can based on the time, resources, and knowledge we have. The\n",
        "idea of clean is really defined by the specific task or concern of your project."
      ]
    }
  ]
}