{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-preparing-movie-review-data-for-sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPO0m9rtDo4tUhFCrPP4BMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-2-bag-of-words/1_preparing_movie_review_data_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvuUyR2EsqvM",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Movie Review Data for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pw9bfors39E",
        "colab_type": "text"
      },
      "source": [
        "Text data preparation is different for each problem. Preparation starts with simple steps, like loading data, but quickly gets difficult with cleaning tasks that are very specific to the data you are working with. You need help as to where to begin and what order to work through the steps from raw data to data ready for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH8rn1SptkcB",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5xpHJqHtlhq",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Trb0ZS1yhQe",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnfwSOJsyiQV",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHoBJJbJz2Mz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "240463ff-1ede-412b-8c9f-4600a7890726"
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 2010, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/2010)\u001b[K\rremote: Counting objects:   1% (21/2010)\u001b[K\rremote: Counting objects:   2% (41/2010)\u001b[K\rremote: Counting objects:   3% (61/2010)\u001b[K\rremote: Counting objects:   4% (81/2010)\u001b[K\rremote: Counting objects:   5% (101/2010)\u001b[K\rremote: Counting objects:   6% (121/2010)\u001b[K\rremote: Counting objects:   7% (141/2010)\u001b[K\rremote: Counting objects:   8% (161/2010)\u001b[K\rremote: Counting objects:   9% (181/2010)\u001b[K\rremote: Counting objects:  10% (201/2010)\u001b[K\rremote: Counting objects:  11% (222/2010)\u001b[K\rremote: Counting objects:  12% (242/2010)\u001b[K\rremote: Counting objects:  13% (262/2010)\u001b[K\rremote: Counting objects:  14% (282/2010)\u001b[K\rremote: Counting objects:  15% (302/2010)\u001b[K\rremote: Counting objects:  16% (322/2010)\u001b[K\rremote: Counting objects:  17% (342/2010)\u001b[K\rremote: Counting objects:  18% (362/2010)\u001b[K\rremote: Counting objects:  19% (382/2010)\u001b[K\rremote: Counting objects:  20% (402/2010)\u001b[K\rremote: Counting objects:  21% (423/2010)\u001b[K\rremote: Counting objects:  22% (443/2010)\u001b[K\rremote: Counting objects:  23% (463/2010)\u001b[K\rremote: Counting objects:  24% (483/2010)\u001b[K\rremote: Counting objects:  25% (503/2010)\u001b[K\rremote: Counting objects:  26% (523/2010)\u001b[K\rremote: Counting objects:  27% (543/2010)\u001b[K\rremote: Counting objects:  28% (563/2010)\u001b[K\rremote: Counting objects:  29% (583/2010)\u001b[K\rremote: Counting objects:  30% (603/2010)\u001b[K\rremote: Counting objects:  31% (624/2010)\u001b[K\rremote: Counting objects:  32% (644/2010)\u001b[K\rremote: Counting objects:  33% (664/2010)\u001b[K\rremote: Counting objects:  34% (684/2010)\u001b[K\rremote: Counting objects:  35% (704/2010)\u001b[K\rremote: Counting objects:  36% (724/2010)\u001b[K\rremote: Counting objects:  37% (744/2010)\u001b[K\rremote: Counting objects:  38% (764/2010)\u001b[K\rremote: Counting objects:  39% (784/2010)\u001b[K\rremote: Counting objects:  40% (804/2010)\u001b[K\rremote: Counting objects:  41% (825/2010)\u001b[K\rremote: Counting objects:  42% (845/2010)\u001b[K\rremote: Counting objects:  43% (865/2010)\u001b[K\rremote: Counting objects:  44% (885/2010)\u001b[K\rremote: Counting objects:  45% (905/2010)\u001b[K\rremote: Counting objects:  46% (925/2010)\u001b[K\rremote: Counting objects:  47% (945/2010)\u001b[K\rremote: Counting objects:  48% (965/2010)\u001b[K\rremote: Counting objects:  49% (985/2010)\u001b[K\rremote: Counting objects:  50% (1005/2010)\u001b[K\rremote: Counting objects:  51% (1026/2010)\u001b[K\rremote: Counting objects:  52% (1046/2010)\u001b[K\rremote: Counting objects:  53% (1066/2010)\u001b[K\rremote: Counting objects:  54% (1086/2010)\u001b[K\rremote: Counting objects:  55% (1106/2010)\u001b[K\rremote: Counting objects:  56% (1126/2010)\u001b[K\rremote: Counting objects:  57% (1146/2010)\u001b[K\rremote: Counting objects:  58% (1166/2010)\u001b[K\rremote: Counting objects:  59% (1186/2010)\u001b[K\rremote: Counting objects:  60% (1206/2010)\u001b[K\rremote: Counting objects:  61% (1227/2010)\u001b[K\rremote: Counting objects:  62% (1247/2010)\u001b[K\rremote: Counting objects:  63% (1267/2010)\u001b[K\rremote: Counting objects:  64% (1287/2010)\u001b[K\rremote: Counting objects:  65% (1307/2010)\u001b[K\rremote: Counting objects:  66% (1327/2010)\u001b[K\rremote: Counting objects:  67% (1347/2010)\u001b[K\rremote: Counting objects:  68% (1367/2010)\u001b[K\rremote: Counting objects:  69% (1387/2010)\u001b[K\rremote: Counting objects:  70% (1407/2010)\u001b[K\rremote: Counting objects:  71% (1428/2010)\u001b[K\rremote: Counting objects:  72% (1448/2010)\u001b[K\rremote: Counting objects:  73% (1468/2010)\u001b[K\rremote: Counting objects:  74% (1488/2010)\u001b[K\rremote: Counting objects:  75% (1508/2010)\u001b[K\rremote: Counting objects:  76% (1528/2010)\u001b[K\rremote: Counting objects:  77% (1548/2010)\u001b[K\rremote: Counting objects:  78% (1568/2010)\u001b[K\rremote: Counting objects:  79% (1588/2010)\u001b[K\rremote: Counting objects:  80% (1608/2010)\u001b[K\rremote: Counting objects:  81% (1629/2010)\u001b[K\rremote: Counting objects:  82% (1649/2010)\u001b[K\rremote: Counting objects:  83% (1669/2010)\u001b[K\rremote: Counting objects:  84% (1689/2010)\u001b[K\rremote: Counting objects:  85% (1709/2010)\u001b[K\rremote: Counting objects:  86% (1729/2010)\u001b[K\rremote: Counting objects:  87% (1749/2010)\u001b[K\rremote: Counting objects:  88% (1769/2010)\u001b[K\rremote: Counting objects:  89% (1789/2010)\u001b[K\rremote: Counting objects:  90% (1809/2010)\u001b[K\rremote: Counting objects:  91% (1830/2010)\u001b[K\rremote: Counting objects:  92% (1850/2010)\u001b[K\rremote: Counting objects:  93% (1870/2010)\u001b[K\rremote: Counting objects:  94% (1890/2010)\u001b[K\rremote: Counting objects:  95% (1910/2010)\u001b[K\rremote: Counting objects:  96% (1930/2010)\u001b[K\rremote: Counting objects:  97% (1950/2010)\u001b[K\rremote: Counting objects:  98% (1970/2010)\u001b[K\rremote: Counting objects:  99% (1990/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010)\u001b[K\rremote: Counting objects: 100% (2010/2010), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/2009)\u001b[K\rremote: Compressing objects:   1% (21/2009)\u001b[K\rremote: Compressing objects:   2% (41/2009)\u001b[K\rremote: Compressing objects:   3% (61/2009)\u001b[K\rremote: Compressing objects:   4% (81/2009)\u001b[K\rremote: Compressing objects:   5% (101/2009)\u001b[K\rremote: Compressing objects:   6% (121/2009)\u001b[K\rremote: Compressing objects:   7% (141/2009)\u001b[K\rremote: Compressing objects:   8% (161/2009)\u001b[K\rremote: Compressing objects:   9% (181/2009)\u001b[K\rremote: Compressing objects:  10% (201/2009)\u001b[K\rremote: Compressing objects:  11% (221/2009)\u001b[K\rremote: Compressing objects:  12% (242/2009)\u001b[K\rremote: Compressing objects:  13% (262/2009)\u001b[K\rremote: Compressing objects:  14% (282/2009)\u001b[K\rremote: Compressing objects:  15% (302/2009)\u001b[K\rremote: Compressing objects:  16% (322/2009)\u001b[K\rremote: Compressing objects:  17% (342/2009)\u001b[K\rremote: Compressing objects:  18% (362/2009)\u001b[K\rremote: Compressing objects:  19% (382/2009)\u001b[K\rremote: Compressing objects:  20% (402/2009)\u001b[K\rremote: Compressing objects:  21% (422/2009)\u001b[K\rremote: Compressing objects:  22% (442/2009)\u001b[K\rremote: Compressing objects:  23% (463/2009)\u001b[K\rremote: Compressing objects:  24% (483/2009)\u001b[K\rremote: Compressing objects:  25% (503/2009)\u001b[K\rremote: Compressing objects:  26% (523/2009)\u001b[K\rremote: Compressing objects:  27% (543/2009)\u001b[K\rremote: Compressing objects:  28% (563/2009)\u001b[K\rremote: Compressing objects:  29% (583/2009)\u001b[K\rremote: Compressing objects:  30% (603/2009)\u001b[K\rremote: Compressing objects:  31% (623/2009)\u001b[K\rremote: Compressing objects:  32% (643/2009)\u001b[K\rremote: Compressing objects:  33% (663/2009)\u001b[K\rremote: Compressing objects:  34% (684/2009)\u001b[K\rremote: Compressing objects:  35% (704/2009)\u001b[K\rremote: Compressing objects:  36% (724/2009)\u001b[K\rremote: Compressing objects:  37% (744/2009)\u001b[K\rremote: Compressing objects:  38% (764/2009)\u001b[K\rremote: Compressing objects:  39% (784/2009)\u001b[K\rremote: Compressing objects:  40% (804/2009)\u001b[K\rremote: Compressing objects:  41% (824/2009)\u001b[K\rremote: Compressing objects:  42% (844/2009)\u001b[K\rremote: Compressing objects:  43% (864/2009)\u001b[K\rremote: Compressing objects:  44% (884/2009)\u001b[K\rremote: Compressing objects:  45% (905/2009)\u001b[K\rremote: Compressing objects:  46% (925/2009)\u001b[K\rremote: Compressing objects:  47% (945/2009)\u001b[K\rremote: Compressing objects:  48% (965/2009)\u001b[K\rremote: Compressing objects:  49% (985/2009)\u001b[K\rremote: Compressing objects:  50% (1005/2009)\u001b[K\rremote: Compressing objects:  51% (1025/2009)\u001b[K\rremote: Compressing objects:  52% (1045/2009)\u001b[K\rremote: Compressing objects:  53% (1065/2009)\u001b[K\rremote: Compressing objects:  54% (1085/2009)\u001b[K\rremote: Compressing objects:  55% (1105/2009)\u001b[K\rremote: Compressing objects:  56% (1126/2009)\u001b[K\rremote: Compressing objects:  57% (1146/2009)\u001b[K\rremote: Compressing objects:  58% (1166/2009)\u001b[K\rremote: Compressing objects:  59% (1186/2009)\u001b[K\rremote: Compressing objects:  60% (1206/2009)\u001b[K\rremote: Compressing objects:  61% (1226/2009)\u001b[K\rremote: Compressing objects:  62% (1246/2009)\u001b[K\rremote: Compressing objects:  63% (1266/2009)\u001b[K\rremote: Compressing objects:  64% (1286/2009)\u001b[K\rremote: Compressing objects:  65% (1306/2009)\u001b[K\rremote: Compressing objects:  66% (1326/2009)\u001b[K\rremote: Compressing objects:  67% (1347/2009)\u001b[K\rremote: Compressing objects:  68% (1367/2009)\u001b[K\rremote: Compressing objects:  69% (1387/2009)\u001b[K\rremote: Compressing objects:  70% (1407/2009)\u001b[K\rremote: Compressing objects:  71% (1427/2009)\u001b[K\rremote: Compressing objects:  72% (1447/2009)\u001b[K\rremote: Compressing objects:  73% (1467/2009)\u001b[K\rremote: Compressing objects:  74% (1487/2009)\u001b[K\rremote: Compressing objects:  75% (1507/2009)\u001b[K\rremote: Compressing objects:  76% (1527/2009)\u001b[K\rremote: Compressing objects:  77% (1547/2009)\u001b[K\rremote: Compressing objects:  78% (1568/2009)\u001b[K\rremote: Compressing objects:  79% (1588/2009)\u001b[K\rremote: Compressing objects:  80% (1608/2009)\u001b[K\rremote: Compressing objects:  81% (1628/2009)\u001b[K\rremote: Compressing objects:  82% (1648/2009)\u001b[K\rremote: Compressing objects:  83% (1668/2009)\u001b[K\rremote: Compressing objects:  84% (1688/2009)\u001b[K\rremote: Compressing objects:  85% (1708/2009)\u001b[K\rremote: Compressing objects:  86% (1728/2009)\u001b[K\rremote: Compressing objects:  87% (1748/2009)\u001b[K\rremote: Compressing objects:  88% (1768/2009)\u001b[K\rremote: Compressing objects:  89% (1789/2009)\u001b[K\rremote: Compressing objects:  90% (1809/2009)\u001b[K\rremote: Compressing objects:  91% (1829/2009)\u001b[K\rremote: Compressing objects:  92% (1849/2009)\u001b[K\rremote: Compressing objects:  93% (1869/2009)\u001b[K\rremote: Compressing objects:  94% (1889/2009)\u001b[K\rremote: Compressing objects:  95% (1909/2009)\u001b[K\rremote: Compressing objects:  96% (1929/2009)\u001b[K\rremote: Compressing objects:  97% (1949/2009)\u001b[K\rremote: Compressing objects:  98% (1969/2009)\u001b[K\rremote: Compressing objects:  99% (1989/2009)\u001b[K\rremote: Compressing objects: 100% (2009/2009)\u001b[K\rremote: Compressing objects: 100% (2009/2009), done.\u001b[K\n",
            "Receiving objects:   0% (1/2010)   \rReceiving objects:   1% (21/2010)   \rReceiving objects:   2% (41/2010)   \rReceiving objects:   3% (61/2010)   \rReceiving objects:   4% (81/2010)   \rReceiving objects:   5% (101/2010)   \rReceiving objects:   6% (121/2010)   \rReceiving objects:   7% (141/2010)   \rReceiving objects:   8% (161/2010)   \rReceiving objects:   9% (181/2010)   \rReceiving objects:  10% (201/2010)   \rReceiving objects:  11% (222/2010)   \rReceiving objects:  12% (242/2010)   \rReceiving objects:  13% (262/2010)   \rReceiving objects:  14% (282/2010)   \rReceiving objects:  15% (302/2010)   \rReceiving objects:  16% (322/2010)   \rReceiving objects:  17% (342/2010)   \rReceiving objects:  18% (362/2010)   \rReceiving objects:  19% (382/2010)   \rReceiving objects:  20% (402/2010)   \rReceiving objects:  21% (423/2010)   \rReceiving objects:  22% (443/2010)   \rReceiving objects:  23% (463/2010)   \rReceiving objects:  24% (483/2010)   \rReceiving objects:  25% (503/2010)   \rReceiving objects:  26% (523/2010)   \rReceiving objects:  27% (543/2010)   \rReceiving objects:  28% (563/2010)   \rReceiving objects:  29% (583/2010)   \rReceiving objects:  30% (603/2010)   \rReceiving objects:  31% (624/2010)   \rReceiving objects:  32% (644/2010)   \rReceiving objects:  33% (664/2010)   \rReceiving objects:  34% (684/2010)   \rReceiving objects:  35% (704/2010)   \rReceiving objects:  36% (724/2010)   \rReceiving objects:  37% (744/2010)   \rReceiving objects:  38% (764/2010)   \rReceiving objects:  39% (784/2010)   \rReceiving objects:  40% (804/2010)   \rReceiving objects:  41% (825/2010)   \rReceiving objects:  42% (845/2010)   \rReceiving objects:  43% (865/2010)   \rReceiving objects:  44% (885/2010)   \rReceiving objects:  45% (905/2010)   \rReceiving objects:  46% (925/2010)   \rReceiving objects:  47% (945/2010)   \rReceiving objects:  48% (965/2010)   \rReceiving objects:  49% (985/2010)   \rReceiving objects:  50% (1005/2010)   \rReceiving objects:  51% (1026/2010)   \rReceiving objects:  52% (1046/2010)   \rReceiving objects:  53% (1066/2010)   \rReceiving objects:  54% (1086/2010)   \rReceiving objects:  55% (1106/2010)   \rReceiving objects:  56% (1126/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  57% (1146/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  58% (1166/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  59% (1186/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  60% (1206/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  61% (1227/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  62% (1247/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  63% (1267/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  64% (1287/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  65% (1307/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  66% (1327/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  67% (1347/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  68% (1367/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  69% (1387/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  70% (1407/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  71% (1428/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  72% (1448/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  73% (1468/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  74% (1488/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  75% (1508/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  76% (1528/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  77% (1548/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  78% (1568/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  79% (1588/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  80% (1608/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  81% (1629/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  82% (1649/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  83% (1669/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  84% (1689/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  85% (1709/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  86% (1729/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  87% (1749/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  88% (1769/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  89% (1789/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  90% (1809/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  91% (1830/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  92% (1850/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  93% (1870/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  94% (1890/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  95% (1910/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  96% (1930/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  97% (1950/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects:  98% (1970/2010), 1.91 MiB | 3.81 MiB/s   \rremote: Total 2010 (delta 1), reused 2009 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects:  99% (1990/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects: 100% (2010/2010), 1.91 MiB | 3.81 MiB/s   \rReceiving objects: 100% (2010/2010), 3.55 MiB | 6.12 MiB/s, done.\n",
            "Resolving deltas:   0% (0/1)   \rResolving deltas: 100% (1/1)   \rResolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TpvrA5Dz5jc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c145a269-810d-4798-9db7-59ff0fd582df"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tmachine-learning-datasets  sample_data\ttxt_sentoken\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iloOxVYv8L7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebab18bb-443a-46dc-d263-d135ad79f159"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# load one file\n",
        "filename = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# see top 5 char\n",
        "text[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'plot '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5EOQlqT-upq",
        "colab_type": "text"
      },
      "source": [
        "We have two directories each with 1,000 documents each. We can process each directory in turn by first getting a list of files in the directory using the listdir() function, then loading each file in turn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_XVXyI99gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith('.txt'):\n",
        "      next\n",
        "\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "\n",
        "    # load document\n",
        "    doc = load_doc(path)\n",
        "    print(f'Loaded {filename}')\n",
        "\n",
        "# specify directory to load\n",
        "directory = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/neg'\n",
        "process_docs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s5-RCjQBL_u",
        "colab_type": "text"
      },
      "source": [
        "## Clean Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRwz_UtbBM5b",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at what data cleaning we might want to do to the movie review\n",
        "data. We will assume that we will be using a bag-of-words model or perhaps a word embedding\n",
        "that does not require too much preparation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F0CkadrBrb2",
        "colab_type": "text"
      },
      "source": [
        "### Split into Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4KtVhfkBsXk",
        "colab_type": "text"
      },
      "source": [
        "We can use the split() function to split the loaded document into tokens separated by white space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O37cnMMm_6Nq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9b52cde0-057b-46fd-ed2d-8b38a6329567"
      },
      "source": [
        "# split into tokens by white space\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY1wyUcOCCMx",
        "colab_type": "text"
      },
      "source": [
        "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
        "Remove punctuation from words (e.g. `what's').\n",
        "* Removing tokens that are just punctuation (e.g. `-').\n",
        "* Removing tokens that contain numbers (e.g. `10/10').\n",
        "* Remove tokens that have one character (e.g. `a').\n",
        "* Remove tokens that don't have much meaning (e.g. `and').\n",
        "\n",
        "Some ideas:\n",
        "* We can filter out punctuation from tokens using regular expressions.\n",
        "* We can remove tokens that are just punctuation or contain numbers by using an isalpha()\n",
        "check on each token.\n",
        "*  We can remove English stop words using the list loaded using NLTK.\n",
        "*  We can filter out short tokens by checking their length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWYpJny7ERUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "170a30bd-f0bf-4214-9333-8884b2942451"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2cATTHMB4wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8AupSMzD-HH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "021ab106-f194-48e9-84e4-df2251560359"
      },
      "source": [
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXgMWlxGFfA_",
        "colab_type": "text"
      },
      "source": [
        "## Develop Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CsY4g4hFhol",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImhSn5EcEJ29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}