{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-preparing-movie-review-data-for-sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNK5sQe5CiCrPNy3N7E0BQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-2-bag-of-words/1_preparing_movie_review_data_for_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvuUyR2EsqvM",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Movie Review Data for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pw9bfors39E",
        "colab_type": "text"
      },
      "source": [
        "Text data preparation is different for each problem. Preparation starts with simple steps, like loading data, but quickly gets difficult with cleaning tasks that are very specific to the data you are working with. You need help as to where to begin and what order to work through the steps from raw data to data ready for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH8rn1SptkcB",
        "colab_type": "text"
      },
      "source": [
        "## Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5xpHJqHtlhq",
        "colab_type": "text"
      },
      "source": [
        "The Movie Review Data is a collection of movie reviews retrieved from the imdb.com website in the early 2000s by Bo Pang and Lillian Lee. The reviews were collected and made available as part of their research on natural language processing.\n",
        "\n",
        "The dataset is comprised of 1,000 positive and 1,000 negative movie reviews drawn from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. The authors refer to this dataset as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "* The dataset is comprised of only English reviews.\n",
        "* All text has been converted to lowercase.\n",
        "* There is white space around punctuation like periods, commas, and brackets.\n",
        "* Text has been split into one sentence per line.\n",
        "\n",
        "The data has been used for a few related natural language processing tasks. For classification, the performance of classical models (such as Support Vector Machines) on the data is in the range of high 70% to low 80% (e.g. 78%-to-82%). More sophisticated data preparation may see results as high as 86% with 10-fold cross-validation.\n",
        "\n",
        "\n",
        "After unzipping the file, you will have a directory called txt sentoken with two sub-directories containing the text neg and pos for negative and positive reviews. Reviews are stored\n",
        "one per file with a naming convention from cv000 to cv999 for each of neg and pos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Trb0ZS1yhQe",
        "colab_type": "text"
      },
      "source": [
        "## Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnfwSOJsyiQV",
        "colab_type": "text"
      },
      "source": [
        "we will look at loading individual text files, then processing the directories of filles. We will fetch data from Github repository where we have storred this Movie Review Polarity Dataset and after fetching it will be available in the current working directory in the folder txt sentoken.\n",
        "\n",
        "We can load an individual text file by opening it, reading\n",
        "in the ASCII text, and closing the file. This is standard file handling stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHoBJJbJz2Mz",
        "colab_type": "code",
        "outputId": "f59c9566-9fbc-4b9b-dc0b-87795f26d6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# fetch dataset from github\n",
        "! git clone https://github.com/rahiakela/machine-learning-datasets -b movie-review-polarity-dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'machine-learning-datasets'...\n",
            "remote: Enumerating objects: 2010, done.\u001b[K\n",
            "remote: Counting objects: 100% (2010/2010), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2009/2009), done.\u001b[K\n",
            "remote: Total 2010 (delta 1), reused 2009 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2010/2010), 3.55 MiB | 19.15 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TpvrA5Dz5jc",
        "colab_type": "code",
        "outputId": "6e0c5de1-3d54-47a8-f98d-ca5b3c676202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "machine-learning-datasets  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iloOxVYv8L7d",
        "colab_type": "code",
        "outputId": "d50bd330-6acb-41b8-e0f5-526d21693984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# load one file\n",
        "filename = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "\n",
        "# see top 5 char\n",
        "text[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'plot '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5EOQlqT-upq",
        "colab_type": "text"
      },
      "source": [
        "We have two directories each with 1,000 documents each. We can process each directory in turn by first getting a list of files in the directory using the listdir() function, then loading each file in turn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_XVXyI99gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith('.txt'):\n",
        "      next\n",
        "\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "\n",
        "    # load document\n",
        "    doc = load_doc(path)\n",
        "    print(f'Loaded {filename}')\n",
        "\n",
        "# specify directory to load\n",
        "directory = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/neg'\n",
        "process_docs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s5-RCjQBL_u",
        "colab_type": "text"
      },
      "source": [
        "## Clean Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRwz_UtbBM5b",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will look at what data cleaning we might want to do to the movie review\n",
        "data. We will assume that we will be using a bag-of-words model or perhaps a word embedding\n",
        "that does not require too much preparation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F0CkadrBrb2",
        "colab_type": "text"
      },
      "source": [
        "### Split into Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4KtVhfkBsXk",
        "colab_type": "text"
      },
      "source": [
        "We can use the split() function to split the loaded document into tokens separated by white space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O37cnMMm_6Nq",
        "colab_type": "code",
        "outputId": "1c7d91c0-b64f-45c2-bdcb-31207bf45bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# split into tokens by white space\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY1wyUcOCCMx",
        "colab_type": "text"
      },
      "source": [
        "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
        "Remove punctuation from words (e.g. `what's').\n",
        "* Removing tokens that are just punctuation (e.g. `-').\n",
        "* Removing tokens that contain numbers (e.g. `10/10').\n",
        "* Remove tokens that have one character (e.g. `a').\n",
        "* Remove tokens that don't have much meaning (e.g. `and').\n",
        "\n",
        "Some ideas:\n",
        "* We can filter out punctuation from tokens using regular expressions.\n",
        "* We can remove tokens that are just punctuation or contain numbers by using an isalpha()\n",
        "check on each token.\n",
        "*  We can remove English stop words using the list loaded using NLTK.\n",
        "*  We can filter out short tokens by checking their length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWYpJny7ERUw",
        "colab_type": "code",
        "outputId": "d7ff9d50-4902-4144-bfc1-4cdca4e2cfd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2cATTHMB4wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8AupSMzD-HH",
        "colab_type": "code",
        "outputId": "35ac679d-0197-44a8-988d-5233965f14ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXgMWlxGFfA_",
        "colab_type": "text"
      },
      "source": [
        "## Develop Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CsY4g4hFhol",
        "colab_type": "text"
      },
      "source": [
        "When working with predictive models of text, like a bag-of-words model, there is a pressure to reduce the size of the vocabulary. The larger the vocabulary, the more sparse the representation\n",
        "of each word or document. A part of preparing text for sentiment analysis involves defining and tailoring the vocabulary of words supported by the model. We can do this by loading all of the\n",
        "documents in the dataset and building a set of words. We may decide to support all of these words, or perhaps discard some. The final chosen vocabulary can then be saved to file for later\n",
        "use, such as filtering words in new documents in the future.\n",
        "\n",
        "We can keep track of the vocabulary in a Counter, which is a dictionary of words and their count with some additional convenience functions. We need to develop a new function to process a document and add it to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImhSn5EcEJ29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3JHYq9yTtF4",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can use our template above for processing all documents in a directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jVec0_0TmOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith('.txt'):\n",
        "      next\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttoD6Kt8UZFK",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together and develop a full vocabulary from all documents in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGYneTkVUPuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dce8d523-2788-492c-de16-93303c0b566e"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "# add all docs to vocab\n",
        "process_docs('machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/neg', vocab)\n",
        "process_docs('machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken/pos', vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46557\n",
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNsaApnVMI8",
        "colab_type": "text"
      },
      "source": [
        "We can see that there are a little over 46,000 unique words across all reviews and the top 3 words are film, one, and movie.\n",
        "\n",
        "Perhaps the least common words, those that only appear once across all reviews, are not predictive. Perhaps some of the most common words are not useful too. These are good questions and really should be tested with a specific predictive model.\n",
        "\n",
        "Generally, words that only appear once or a few times across 2,000 reviews are probably not predictive and can be removed from the vocabulary, greatly cutting down on the tokens we need to model. We can do this by stepping through words and their counts and only keeping those with a count above a chosen threshold. \n",
        "\n",
        "Here we will use 5 occurrences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6UeDpVHVB2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22999545-9ae2-4701-c557-c40f469fd904"
      },
      "source": [
        "# keep tokens with > 5 occurrence\n",
        "min_occurane = 5\n",
        "tokens = [k for k, c in vocab.items() if c > min_occurane]\n",
        "print(len(tokens))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QMWDtIgWoYH",
        "colab_type": "text"
      },
      "source": [
        "This reduces the vocabulary from 46,557 to 13,058 words, a huge drop. Perhaps a minimum of 5 occurrences is too aggressive; you can experiment with diferent values. We can then save the chosen vocabulary of words to a new file. I like to save the vocabulary as ASCII with one word per line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zK2vygQWaap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_list(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnBef3yBXJZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnLJiTWFXnIs",
        "colab_type": "text"
      },
      "source": [
        "## Save Prepared Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFASWTbXoIl",
        "colab_type": "text"
      },
      "source": [
        "We can use the data cleaning and chosen vocabulary to prepare each movie review and save the prepared versions of the reviews ready for modeling. \n",
        "\n",
        "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas. We can start off by loading the vocabulary from vocab.txt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25oyauJjXX7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sGYT7JIYlfv",
        "colab_type": "text"
      },
      "source": [
        "Next, we can clean the reviews, use the loaded vocab to filter out unwanted tokens, and save the clean reviews in a new file.\n",
        "\n",
        "One approach could be to save all the positive reviews in one file and all the negative reviews in another file, with the filtered tokens separated by white space for each review on separate lines. \n",
        "\n",
        "First, we can define a function to process a document, clean it, filter it, and return it as a single line that could be saved in a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLXM0zODYaII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  # load the doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # filter by vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTG8f1ZFZXcM",
        "colab_type": "text"
      },
      "source": [
        "Next, we can define a new version of process docs() to step through all reviews in a folder and convert them to lines by calling doc to line() for each document. A list of lines is then\n",
        "returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Repxp-2nZRvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "  lines = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith('.txt'):\n",
        "      next\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "\n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQcARMhKZ8EG",
        "colab_type": "text"
      },
      "source": [
        "We can then call process docs() for both the directories of positive and negative reviews, then call save list() from the previous section to save each list of processed reviews to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKDRCQ_5Z2zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "\n",
        "base_path = 'machine-learning-datasets/movie-review-polarity-dataset/txt_sentoken'\n",
        "\n",
        "# prepare negative reviews\n",
        "negative_lines = process_docs(base_path + '/neg', vocab)\n",
        "save_list(negative_lines, 'negative.txt')\n",
        "\n",
        "# prepare positive reviews\n",
        "positive_lines = process_docs(base_path + '/pos', vocab)\n",
        "save_list(positive_lines, 'positive.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_hmDcPbowR",
        "colab_type": "text"
      },
      "source": [
        "Running this saves two new files, negative.txt and positive.txt, that contain the prepared negative and positive reviews respectively. \n",
        "\n",
        "The data is ready for use in a bag-of-words or even word embedding model."
      ]
    }
  ]
}