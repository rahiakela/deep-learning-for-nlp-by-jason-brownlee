{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-preparing-text-data-with-scikit-learn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8/I3BdyykLg6tovzb0hE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-1-data-preparation/2_preparing_text_data_with_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqWktSqmgPUg",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Text Data with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kHwc2Pygh7O",
        "colab_type": "text"
      },
      "source": [
        "Text data requires special preparation before you can start using it for predictive modeling. The text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or \n",
        "oating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization). The scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9BiqOCyguKM",
        "colab_type": "text"
      },
      "source": [
        "## The Bag-of-Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cDNVpFjgvSX",
        "colab_type": "text"
      },
      "source": [
        "We cannot work with text directly when using machine learning algorithms. Instead, we need to convert the text to numbers. We may want to perform classification of documents, so each document is an input and a class label is the output for our predictive algorithm. Algorithms\n",
        "take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\n",
        "\n",
        "A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model.The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document. This can be done by assigning each word a unique number. Then any document we see can be encoded\n",
        "as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.\n",
        "\n",
        "This is the bag-of-words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order. There are many ways to extend this simple method, both by better clarifying what a word is and in defining what to encode about each word in the vector. \n",
        "\n",
        "The scikit-learn library provides 3 different schemes that we can use:-\n",
        "* CountVectorizer for Word Counts\n",
        "* TfidfVectorizer for Word Frequencies\n",
        "* HashingVectorizer for Hashing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVWg9k7oh036",
        "colab_type": "text"
      },
      "source": [
        "## Word Counts with CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZJ8qJgBh6wu",
        "colab_type": "text"
      },
      "source": [
        "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3dNgawLiLIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "56a20d09-4967-4e51-c394-98c3ac0ccf34"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = ['The quick brown fox jumped over the lazy dog.']\n",
        "\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# showing shape of vector\n",
        "print(vector.shape)\n",
        "\n",
        "# showing type vector\n",
        "print(type(vector))\n",
        "\n",
        "# showing a count of occurrence for each word\n",
        "print(vector.toarray())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "(1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvKndaLjnXe",
        "colab_type": "text"
      },
      "source": [
        "Importantly, the same vectorizer can be used on documents that contain words not included\n",
        "in the vocabulary. These words are ignored and no count is given in the resulting vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fVeBxqlitMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e87ba25a-bf84-4cc4-e24c-c3d474715e01"
      },
      "source": [
        "# encode another document\n",
        "text2 = ['the puppy']\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXyn6i6uj5O1",
        "colab_type": "text"
      },
      "source": [
        "## Word Frequencies with TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjmNxHEvj6Ns",
        "colab_type": "text"
      },
      "source": [
        "Word counts are a good starting point, but are very basic. One issue with simple counts is that\n",
        "some words like the will appear many times and their large counts will not be very meaningful\n",
        "in the encoded vectors. An alternative is to calculate word frequencies, and by far the most\n",
        "popular method is called **TF-IDF**. This is an acronym that stands for **Term Frequency - Inverse\n",
        "Document Frequency** which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "* Term Frequency: This summarizes how often a given word appears within a document.\n",
        "* Inverse Document Frequency:This downscales words that appear a lot across documents.\n",
        "\n",
        "**TF-IDF** are word frequency scores that try to highlight\n",
        "words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "The **TfidfVectorizer** will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. \n",
        "\n",
        "Alternately, if you already have a\n",
        "learned **CountVectorizer**, you can use it with a TfidfTransformer to just calculate the inverse\n",
        "document frequencies and start encoding documents. The same create, fit, and transform process\n",
        "is used as with the **CountVectorizer**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY49hPsGj0qn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "3f2cdcc2-c119-49de-c8e5-39332ae82e39"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\n",
        "  'The quick brown fox jumped over the lazy dog.',\n",
        "  'The dog',\n",
        "  'The fox'\n",
        "]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "(1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmmHrXMgl5-V",
        "colab_type": "text"
      },
      "source": [
        "The scores are normalized to values between 0 and 1 and the encoded document vectors can\n",
        "then be used directly with most machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZNWLMK-l63g",
        "colab_type": "text"
      },
      "source": [
        "## Hashing with HashingVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgnVgOiBl-aH",
        "colab_type": "text"
      },
      "source": [
        "Counts and frequencies can be very useful, but one limitation of these methods is that the\n",
        "vocabulary can become very large. This, in turn, will require large vectors for encoding\n",
        "documents and impose large requirements on memory and slow down algorithms. A clever work\n",
        "around is to use a one way hash of words to convert them to integers. The clever part is that\n",
        "no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word.\n",
        "\n",
        "The **HashingVectorizer** class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n",
        "\n",
        "An arbitrary fixed-length vector size\n",
        "of 20 was chosen. This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhhEQELilqrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c2482c5e-bc6d-43ae-b27d-98303ae814c9"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = ['The quick brown fox jumped over the lazy dog.']\n",
        "\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n",
            "[[ 0.          0.          0.          0.          0.          0.33333333\n",
            "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
            "   0.          0.          0.         -0.33333333  0.          0.\n",
            "  -0.66666667  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}