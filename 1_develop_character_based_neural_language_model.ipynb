{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_develop_character_based_neural_language_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-5-language-modeling/1_develop_character_based_neural_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop a Character-Based Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence. It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small\n",
        "vocabulary and  exibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train. \n",
        "\n",
        "Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling.\n",
        "\n",
        "We will cover the followings topics:-\n",
        "\n",
        "* Prepare text for character-based language modeling.\n",
        "* Develop a character-based language model using LSTMs.\n",
        "* Use a trained character-based language model to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Sing a Song of Sixpence Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The nursery rhyme Sing a Song of Sixpence is well known in the west. The first verse is common, but there is also a 4 verse version that we will use to develop our character-based language model. It is short, so fitting the model will be fast, but not so short that we won't see anything\n",
        "interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "The first step is to prepare the text data. We will start by defining the type of language model.:\n",
        "\n",
        "1. Language Model Design.\n",
        "2. Load Text.\n",
        "3. Clean Text.\n",
        "4. Create Sequences\n",
        "5. Save Sequences\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Language Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters. The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character. After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
        "\n",
        "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text.\n",
        "\n",
        "We will use an arbitrary length of 10 characters for this model. There is not a lot of text, and 10 characters is a few words. We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "Tying all of this together, the complete code listing is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUxIuMFB7Hxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "39ace12f-9c3d-4a04-cd65-59a4770fd3de"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# load text\n",
        "raw_text = load_doc('rhyme.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean text\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "\n",
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "  seq = raw_text[i - length: i + 1]   # select sequence of tokens\n",
        "  sequences.append(seq)\n",
        "print(f'\\nTotal Sequences: {str(len(sequences))}')\n",
        "\n",
        "# save sequences to file\n",
        "print(sequences[:10])\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,\n",
            "A pocket full of rye.\n",
            "Four and twenty blackbirds,\n",
            "Baked in a pie.\n",
            "\n",
            "When the pie was opened\n",
            "The birds began to sing;\n",
            "Wasn't that a dainty dish,\n",
            "To set before the king.\n",
            "\n",
            "The king was in his counting house,\n",
            "Counting out his money;\n",
            "The queen was in the parlour,\n",
            "Eating bread and honey.\n",
            "\n",
            "The maid was in the garden,\n",
            "Hanging out the clothes,\n",
            "When down came a blackbird\n",
            "And pecked off her nose.\n",
            "\n",
            "Total Sequences: 399\n",
            "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ', 'a song of s', ' song of si', 'song of six', 'ong of sixp', 'ng of sixpe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkztLwOlCqTG",
        "colab_type": "text"
      },
      "source": [
        "## Train Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Clean All Reviews and Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "We can now use the function to clean reviews and apply it to all reviews. To do this, we will develop a new function named process docs() below that will walk through all reviews in a directory, clean them, and return them as a list. We will also add an argument to the function to indicate whether the function is processing train or test reviews, that way the filenames can\n",
        "be filtered and only those train or test reviews requested will be cleaned\n",
        "and returned.\n",
        "\n",
        "```python\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "```\n",
        "\n",
        "We can call this function with negative training reviews. We also need labels for the train and test documents. We know that we have 900 training documents and 100 test documents. We can use a Python list comprehension to create the labels for the negative (0) and positive (1) reviews for both train and test sets.\n",
        "\n",
        "```python\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "  # load documents\n",
        "  neg = process_docs('txt_sentoken/neg', is_train)\n",
        "  pos = process_docs('txt_sentoken/pos', is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "Finally, we want to save the prepared train and test sets to file so that we can load them later for modeling and model evaluation.\n",
        "\n",
        "```python\n",
        "def save_dataset(lines, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "outputId": "020f8ec9-053a-4d9e-fd6a-9e9b89550207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  tokens = ' '.join(tokens)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue \n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # add to list\n",
        "    documents.append(documents)\n",
        "\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "  # load documents\n",
        "  neg = process_docs(base_path + '/neg', is_train)\n",
        "  pos = process_docs(base_path + '/pos', is_train)\n",
        "\n",
        "  docs = neg + pos\n",
        "\n",
        "  # prepare labels\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "\n",
        "  return docs, labels\n",
        "\n",
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print(f'Saved: {filename}')\n",
        "\n",
        "# load and clean all reviews\n",
        "train_docs, ytrain = load_clean_dataset(True)\n",
        "test_docs, ytest = load_clean_dataset(False)\n",
        "\n",
        "# save training datasets\n",
        "save_dataset([train_docs, ytrain], 'train.pkl')\n",
        "save_dataset([test_docs, ytest], 'test.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9a4PZI02Qs",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to develop our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Develop Multichannel Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5a8rWe353s",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will develop a multichannel convolutional neural network for the sentiment analysis prediction problem. This section is divided into 3 parts:\n",
        "\n",
        "1.  Encode Data\n",
        "2.  Define Model.\n",
        "3.  Complete Example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ifwli636pl",
        "colab_type": "text"
      },
      "source": [
        "### Encode Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "The first step is to load the cleaned training dataset. The function below-named load dataset() can be called to load the pickled training dataset.\n",
        "\n",
        "\n",
        "```python\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "  return load(open(filename, 'rb'))\n",
        "\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "```\n",
        "\n",
        "Next, we must fit a Keras Tokenizer on the training dataset. We will use this tokenizer to both define the vocabulary for the Embedding layer and encode the review documents as integers.\n",
        "\n",
        "```python\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "```\n",
        "\n",
        "We also need to know the maximum length of input sequences as input for the model and to pad all sequences to the fixed length.\n",
        "\n",
        "The function max length() below will calculate the maximum length (number of words) for all reviews in the training dataset.\n",
        "\n",
        "```python\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "  return max([len(s.split()) for s in lines])\n",
        "```\n",
        "\n",
        "We also need to know the size of the vocabulary for the Embedding layer. This can be calculated from the prepared Tokenizer.\n",
        "\n",
        "```python\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "```\n",
        "\n",
        "Finally, we can integer encode and pad the clean movie review text. The function below named encode text() will both encode and pad text data to the maximum review length.\n",
        "\n",
        "```python\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad encoded sequences\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOSY4i76vFF",
        "colab_type": "text"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jCLBKNh6xl9",
        "colab_type": "text"
      },
      "source": [
        "A standard model for document classification is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. The kernel size in the convolutional layer defines the number of words to consider as the convolution is passed across the input text document, providing a grouping parameter. \n",
        "\n",
        "A multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels. This allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n",
        "\n",
        "In Keras, a multiple-input model can be defined using the functional API. We will define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. Each channel is comprised of the following elements:\n",
        "\n",
        "* **Input layer** that defines the length of input sequences.\n",
        "* **Embedding layer** set to the size of the vocabulary and 100-dimensional real-valued representations.\n",
        "* **Conv1D layer** with 32 filters and a kernel size set to the number of words to read at once.\n",
        "* **MaxPooling1D layer** to consolidate the output from the convolutional layer.\n",
        "* **Flatten layer** to reduce the three-dimensional output to two dimensional for concatenation.\n",
        "\n",
        "The output from the three channels are concatenated into a single vector and process by a Dense layer and an output layer.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "\n",
        "  # channel 1\n",
        "  inputs1 = Input(shape=(length,))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  # channel 2\n",
        "  inputs2 = Input(shape=(length,))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  # channel 3\n",
        "  inputs3 = Input(shape=(length,))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  # merge\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  # interpretation\n",
        "  dense1 = Dense(10, activation='relu')(merged)\n",
        "  outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\n",
        "  # compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  # summarize\n",
        "  model.summary()\n",
        "  plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "  return model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import load\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def load_dataset(filename):\n",
        "  # load dataset\n",
        "  return load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "  return max([len(line.split()) for line in lines])\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, max_length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "\n",
        "  return padded\n",
        "\n",
        "# define the model\n",
        "def define_model(max_length, vocab_size):\n",
        "\n",
        "  print('Creating channel.......')\n",
        "  # channel 1\n",
        "  inputs1 = Input(shape=(max_length, ))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPool1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  print('Creating channe2.......')\n",
        "  # channel 2\n",
        "  inputs2 = Input(shape=(max_length, ))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPool1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  print('Creating channe3.......')\n",
        "  # channel 3\n",
        "  inputs3 = Input(shape=(max_length, ))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPool1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  print('Creating all channes.......')\n",
        "  # merge all channel\n",
        "  merged_layer = Concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  # interpretation\n",
        "  dense_layer = Dense(10, activation='relu')(merged_layer)\n",
        "  output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "\n",
        "  print('Creating model.......')\n",
        "  # create model\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output_layer)\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "\n",
        "  # plot model architecture\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Loadin dataset.......')\n",
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "# convert to array\n",
        "trainLines = np.array(trainLines)\n",
        "trainLabels = np.array(trainLabels)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "\n",
        "# calculate the maximum document length\n",
        "max_length = max_length(trainLines)\n",
        "print(f'Maximum document length: {str(max_length)}')\n",
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size: {str(vocab_size)}')\n",
        "\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, max_length)\n",
        "\n",
        "print('Creating model.......')\n",
        "# define model\n",
        "model = define_model(max_length, vocab_size)\n",
        "\n",
        "print('Traing model.......')\n",
        "# fit model\n",
        "model.fit([trainX, trainX, trainX], trainLabels, epochs=7, batch_size=16, verbose=1)\n",
        "\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGBg8wz25chd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxYlcVYRN681",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate the fit model by predicting the sentiment on all reviews in the unseen test dataset. Using the data loading functions developed in the previous section, we can load and encode both the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4qYNMzODBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print(f'Max document length: {length}')\n",
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLabels, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate([trainX, trainX, trainX], trainLabels, verbose=0)\n",
        "print(f'Train Accuracy: {acc * 100}')\n",
        "\n",
        "# evaluate model on test dataset dataset\n",
        "_, acc = model.evaluate([testX, testX, testX], testLabels, verbose=0)\n",
        "print(f'Test Accuracy: {acc * 100}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An1T_q5QR7ST",
        "colab_type": "text"
      },
      "source": [
        "We can see that, as expected, the skill on the training dataset is excellent, here at 100% accuracy. We can also see that the skill of the model on the unseen test dataset is also very impressive, achieving 88.5%, which is above the skill of the model reported in the 2014 paper."
      ]
    }
  ]
}