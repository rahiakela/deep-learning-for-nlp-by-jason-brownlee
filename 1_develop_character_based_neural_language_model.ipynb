{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_develop_character_based_neural_language_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-for-nlp-by-jason-brownlee/blob/part-5-language-modeling/1_develop_character_based_neural_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfh6KONLasT2",
        "colab_type": "text"
      },
      "source": [
        "# Develop a Character-Based Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-XEuaxbyJe",
        "colab_type": "text"
      },
      "source": [
        "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence. It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small\n",
        "vocabulary and  exibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train. \n",
        "\n",
        "Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling.\n",
        "\n",
        "We will cover the followings topics:-\n",
        "\n",
        "* Prepare text for character-based language modeling.\n",
        "* Develop a character-based language model using LSTMs.\n",
        "* Use a trained character-based language model to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6DjCOqz9aj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or6TM-Sez-p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pickle import dump\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohtiZJzjlJQ",
        "colab_type": "text"
      },
      "source": [
        "## Sing a Song of Sixpence Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI_4nRavjl0K",
        "colab_type": "text"
      },
      "source": [
        "The nursery rhyme Sing a Song of Sixpence is well known in the west. The first verse is common, but there is also a 4 verse version that we will use to develop our character-based language model. It is short, so fitting the model will be fast, but not so short that we won't see anything\n",
        "interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FS8kSjj6KZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGTKCRWFj6_x",
        "colab_type": "text"
      },
      "source": [
        "The first step is to prepare the text data. We will start by defining the type of language model.:\n",
        "\n",
        "1. Language Model Design.\n",
        "2. Load Text.\n",
        "3. Clean Text.\n",
        "4. Create Sequences\n",
        "5. Save Sequences\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJS3tQakSOe",
        "colab_type": "text"
      },
      "source": [
        "### Language Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTTxAfwrkT34",
        "colab_type": "text"
      },
      "source": [
        "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters. The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character. After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
        "\n",
        "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text.\n",
        "\n",
        "We will use an arbitrary length of 10 characters for this model. There is not a lot of text, and 10 characters is a few words. We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAmwRJvwlFy1",
        "colab_type": "text"
      },
      "source": [
        "Tying all of this together, the complete code listing is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUxIuMFB7Hxg",
        "colab_type": "code",
        "outputId": "44b25779-25d3-47fd-ef58-386f3d14dfe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename, 'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "# load text\n",
        "raw_text = load_doc('rhyme.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean text\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "\n",
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "  seq = raw_text[i - length: i + 1]   # select sequence of tokens\n",
        "  sequences.append(seq)\n",
        "print(f'\\nTotal Sequences: {str(len(sequences))}')\n",
        "\n",
        "# save sequences to file\n",
        "print(sequences[:10])\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,\n",
            "A pocket full of rye.\n",
            "Four and twenty blackbirds,\n",
            "Baked in a pie.\n",
            "\n",
            "When the pie was opened\n",
            "The birds began to sing;\n",
            "Wasn't that a dainty dish,\n",
            "To set before the king.\n",
            "\n",
            "The king was in his counting house,\n",
            "Counting out his money;\n",
            "The queen was in the parlour,\n",
            "Eating bread and honey.\n",
            "\n",
            "The maid was in the garden,\n",
            "Hanging out the clothes,\n",
            "When down came a blackbird\n",
            "And pecked off her nose.\n",
            "\n",
            "Total Sequences: 399\n",
            "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ', 'a song of s', ' song of si', 'song of six', 'ong of sixp', 'ng of sixpe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkztLwOlCqTG",
        "colab_type": "text"
      },
      "source": [
        "## Train Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vi67Za2gdBU",
        "colab_type": "text"
      },
      "source": [
        "We will develop a neural language model for the prepared sequence data. The\n",
        "model will read encoded characters and predict the next character in the sequence. \n",
        "\n",
        "A Long Short-Term Memory recurrent neural network hidden layer will be used to learn the context from the input sequence in order to make the predictions.\n",
        "\n",
        "We will do the following steps:-\n",
        "1. Load Data \n",
        "2. Encode Sequences\n",
        "3. Split Inputs and Output\n",
        "4. Fit Model\n",
        "5. Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uxLKYNshtNq",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIo9-p8vhxB7",
        "colab_type": "text"
      },
      "source": [
        "The first step is to load the prepared character sequence data from char sequences.txt.\n",
        "\n",
        "```python\n",
        "# load\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ytOnGM_h2AB",
        "colab_type": "text"
      },
      "source": [
        "### Encode Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Emc33PiKAZ",
        "colab_type": "text"
      },
      "source": [
        "The sequences of characters must be encoded as integers. This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers. We can create the mapping given a sorted set of unique characters in the raw input data. The mapping is a dictionary of character values to integer values.\n",
        "\n",
        "```python\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "```\n",
        "\n",
        "Next, we can process each sequence of characters one at a time and use the dictionary mapping to look up the integer value for each character.\n",
        "\n",
        "```python\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "  # integer encode line\n",
        "  encoded_seq = [mapping[char] for char in line]\n",
        "  sequences.append(encoded_seq)\n",
        "```\n",
        "\n",
        "The result is a list of integer lists. We need to know the size of the vocabulary later. We can retrieve this as the size of the dictionary mapping.\n",
        "\n",
        "```python\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Lix7FJjv1g",
        "colab_type": "text"
      },
      "source": [
        "### Split Inputs and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmGm44jpoTH7",
        "colab_type": "text"
      },
      "source": [
        "Now that the sequences have been integer encoded, we can separate the columns into input and output sequences of characters. We can do this using a simple array slice.\n",
        "\n",
        "```python\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "```\n",
        "\n",
        "Next, we need to one hot encode each character. That is, each character becomes a vector as long as the vocabulary (38 elements) with a 1 marked for the specific character. This provides a more precise input representation for the network. It also provides a clear objective for the\n",
        "network to predict, where a probability distribution over characters can be output by the model and compared to the ideal case of all 0 values with a 1 for the actual next character. \n",
        "\n",
        "We can use the to categorical() function in the Keras API to one hot encode the input and output sequences.\n",
        "\n",
        "```python\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlNCCBBdj8xP",
        "colab_type": "text"
      },
      "source": [
        "### Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJK7EcAhqois",
        "colab_type": "text"
      },
      "source": [
        "The model is defined with an input layer that takes sequences that have 10 time steps and 38 features for the one hot encoded input sequences. Rather than specify these numbers, we use the second and third dimensions on the X input data. This is so that if we change the length of the sequences or size of the vocabulary, we do not need to change the model definition. \n",
        "\n",
        "The model has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and error. \n",
        "\n",
        "The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. \n",
        "\n",
        "A softmax activation function is used on the output layer to ensure the output has the properties of a probability distribution.\n",
        "\n",
        "```python\n",
        "# define the model\n",
        "def define_model(X):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "```\n",
        "\n",
        "The model is learning a multiclass classification problem, therefore we use the categorical log loss intended for this type of problem. The efficient Adam implementation of gradient descent is used to optimize the model and accuracy is reported at the end of each batch update. The model is fit for 100 training epochs, again found with a little trial and error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJaxjnwJk_eN",
        "colab_type": "text"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvwy7RUlAVE",
        "colab_type": "text"
      },
      "source": [
        "After the model is fit, we save it to file for later use. The Keras model API provides the save() function that we can use to save the model to a single file, including weights and topology information.\n",
        "\n",
        "```python\n",
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "```\n",
        "\n",
        "We also save the mapping from characters to integers that we will need to encode any input when using the model and decode any output from the model.\n",
        "\n",
        "```python\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5POW5fBlm_S",
        "colab_type": "text"
      },
      "source": [
        "### Complete Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vpHFvCglnyw",
        "colab_type": "text"
      },
      "source": [
        "Tying all of this together, the complete code listing for fitting the character-based neural language model is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uSeaYAt3DNt",
        "colab_type": "code",
        "outputId": "03feec04-f825-4264-c1ec-68f967a79f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "\n",
        "\n",
        "# define the model\n",
        "def define_model(X):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "\n",
        "# load\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')\n",
        "\n",
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "  # integer encode line\n",
        "  encoded_seq = [mapping[char] for char in line]\n",
        "  sequences.append(encoded_seq)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print(f'Vocabulary Size: {str(vocab_size)}')\n",
        "\n",
        "# separate into input and output\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = np.array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(X)\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)\n",
        "\n",
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 38\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 75)                34200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 38)                2888      \n",
            "=================================================================\n",
            "Total params: 37,088\n",
            "Trainable params: 37,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 - 0s - loss: 3.6000 - accuracy: 0.0902\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 3.4634 - accuracy: 0.1679\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 3.1387 - accuracy: 0.1905\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 3.0259 - accuracy: 0.1905\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 3.0062 - accuracy: 0.1905\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 2.9777 - accuracy: 0.1905\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 2.9739 - accuracy: 0.1905\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 2.9422 - accuracy: 0.1905\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 2.9471 - accuracy: 0.1905\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 2.9071 - accuracy: 0.1905\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 2.8972 - accuracy: 0.1905\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 2.8565 - accuracy: 0.1905\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 2.8002 - accuracy: 0.1955\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 2.7966 - accuracy: 0.1955\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 2.7509 - accuracy: 0.2256\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 2.7175 - accuracy: 0.2356\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 2.6691 - accuracy: 0.2581\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 2.6121 - accuracy: 0.2581\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 2.5817 - accuracy: 0.2782\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 2.5511 - accuracy: 0.2607\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 2.5043 - accuracy: 0.2832\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 2.4496 - accuracy: 0.2882\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 2.4038 - accuracy: 0.2907\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 2.3634 - accuracy: 0.3308\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 2.3400 - accuracy: 0.3158\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 2.2705 - accuracy: 0.3308\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 2.2462 - accuracy: 0.3584\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 2.2076 - accuracy: 0.3484\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 2.1520 - accuracy: 0.3885\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 2.1248 - accuracy: 0.3509\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 2.0719 - accuracy: 0.4035\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 2.0192 - accuracy: 0.4035\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 2.0419 - accuracy: 0.4010\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 1.9698 - accuracy: 0.4637\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 1.9443 - accuracy: 0.4461\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 1.8766 - accuracy: 0.4662\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 1.8457 - accuracy: 0.4837\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 1.8122 - accuracy: 0.4662\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 1.7774 - accuracy: 0.4812\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 1.7530 - accuracy: 0.4912\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 1.7277 - accuracy: 0.4962\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 1.6612 - accuracy: 0.5113\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 1.6225 - accuracy: 0.5313\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 1.5964 - accuracy: 0.5388\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 1.5280 - accuracy: 0.5564\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 1.5203 - accuracy: 0.5915\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 1.4693 - accuracy: 0.5940\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 1.4526 - accuracy: 0.5840\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 1.4002 - accuracy: 0.6090\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 1.3517 - accuracy: 0.6291\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 1.3270 - accuracy: 0.6291\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 1.3069 - accuracy: 0.6566\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 1.2527 - accuracy: 0.6842\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 1.2233 - accuracy: 0.6867\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 1.1906 - accuracy: 0.6942\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 1.2078 - accuracy: 0.6942\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 1.1668 - accuracy: 0.7118\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 1.1144 - accuracy: 0.7519\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 1.0533 - accuracy: 0.7644\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 1.0378 - accuracy: 0.7895\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 1.0007 - accuracy: 0.7970\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 0.9517 - accuracy: 0.8195\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 0.9191 - accuracy: 0.8145\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 0.9209 - accuracy: 0.8120\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 0.9166 - accuracy: 0.8120\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 0.8724 - accuracy: 0.8446\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 0.8249 - accuracy: 0.8571\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 0.7955 - accuracy: 0.8596\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 0.7823 - accuracy: 0.8822\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 0.7502 - accuracy: 0.8897\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 0.7360 - accuracy: 0.8697\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 0.6996 - accuracy: 0.8947\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 0.6848 - accuracy: 0.9023\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 0.6603 - accuracy: 0.9073\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 0.6350 - accuracy: 0.9073\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 0.6027 - accuracy: 0.9398\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 0.5809 - accuracy: 0.9323\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 0.5613 - accuracy: 0.9424\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 0.5389 - accuracy: 0.9398\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 0.5142 - accuracy: 0.9574\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 0.4950 - accuracy: 0.9574\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 0.4713 - accuracy: 0.9524\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 0.4708 - accuracy: 0.9674\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 0.4427 - accuracy: 0.9649\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 0.4237 - accuracy: 0.9624\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 0.4008 - accuracy: 0.9749\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 0.4008 - accuracy: 0.9699\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 0.3956 - accuracy: 0.9724\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 0.3721 - accuracy: 0.9699\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 0.3511 - accuracy: 0.9774\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 0.3458 - accuracy: 0.9799\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 0.3378 - accuracy: 0.9799\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 0.3173 - accuracy: 0.9749\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 0.2995 - accuracy: 0.9850\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 0.2921 - accuracy: 0.9925\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 0.2747 - accuracy: 0.9875\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 0.2687 - accuracy: 0.9875\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 0.2644 - accuracy: 0.9850\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 0.2592 - accuracy: 0.9900\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 0.2438 - accuracy: 0.9875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt9a4PZI02Qs",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to develop our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfuqzoVrpqb",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5a8rWe353s",
        "colab_type": "text"
      },
      "source": [
        "We will use the learned language model to generate new sequences of text that have the same statistical properties.. This section is divided into 3 parts:\n",
        "\n",
        "1.  Load Model\n",
        "2.  Generate Characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ifwli636pl",
        "colab_type": "text"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxbs0r9rqra",
        "colab_type": "text"
      },
      "source": [
        "The first step is to load the model saved to the file model.h5.\n",
        "\n",
        "\n",
        "```python\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "```\n",
        "\n",
        "We also need to load the pickled dictionary for mapping characters to integers from the file mapping.pkl.\n",
        "\n",
        "```python\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOSY4i76vFF",
        "colab_type": "text"
      },
      "source": [
        "### Generate Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jCLBKNh6xl9",
        "colab_type": "text"
      },
      "source": [
        "We must provide sequences of 10 characters as input to the model in order to start the generation process. We will pick these manually. A given input sequence will need to be prepared in the same way as preparing the training data for the model. \n",
        "\n",
        "First, the sequence of characters must be integer encoded using the loaded mapping.\n",
        "\n",
        "```python\n",
        "# encode the characters as integers\n",
        "encoded = [mapping[char] for char in in_text]\n",
        "```\n",
        "\n",
        "Next, the integers need to be one hot encoded using the to categorical() Keras function. We also need to reshape the sequence to be 3-dimensional, as we only have one sequence and LSTMs require all input to be three dimensional (samples, time steps, features).\n",
        "\n",
        "```python\n",
        "# one hot encode\n",
        "encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
        "```\n",
        "\n",
        "We can then use the model to predict the next character in the sequence. We use predict classes() instead of predict() to directly select the integer for the character with the highest probability instead of getting the full probability distribution across the entire set of characters.\n",
        "\n",
        "```python\n",
        "# predict character\n",
        "yhat = model.predict_classes(encoded, verbose=0)\n",
        "```\n",
        "\n",
        "We can then decode this integer by looking up the mapping to see the character to which it maps.\n",
        "\n",
        "```python\n",
        "out_char = ''\n",
        "for char, index in mapping.items():\n",
        "  if index == yhat:\n",
        "    out_char = char\n",
        "    break\n",
        "```\n",
        "\n",
        "This character can then be added to the input sequence. We then need to make sure that the input sequence is 10 characters by truncating the first character from the input sequence text.\n",
        "\n",
        "We can use the pad sequences() function from the Keras API that can perform this truncation operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2AiLbbG-Rp9",
        "colab_type": "text"
      },
      "source": [
        "### Complete Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405rUsUR4lNf",
        "colab_type": "text"
      },
      "source": [
        "We can put all of this together in a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1FM5YqkU2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "81adbaef-040b-4dcd-98dd-26eda53af083"
      },
      "source": [
        "from pickle import load\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "  in_text = seed_text\n",
        "  # generate a fixed number of characters\n",
        "  for _ in range(n_chars):\n",
        "    # encode the characters as integers\n",
        "    encoded = [mapping[char] for char in in_text]\n",
        "    # truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "    # one hot encode\n",
        "    encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "    encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
        "    # predict character\n",
        "    yhat = model.predict_classes(encoded, verbose=0)\n",
        "    # reverse map integer to character\n",
        "    out_char = ''\n",
        "    for char, index in mapping.items():\n",
        "      if index == yhat:\n",
        "        out_char = char\n",
        "        break\n",
        "    # append to input\n",
        "    in_text += out_char\n",
        "  return in_text\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpQ-yJJJA6uV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "e1c47ca7-0a2f-48cb-a74c-8e71771d3b79"
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-738451338407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sing a son'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8193bfc24ef6>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# one hot encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# predict character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 380 into shape (1,1,10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ_L7kZCEjP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "7c82256d-4634-4661-ee24-ed97cbac2886"
      },
      "source": [
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a67fd483dd37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king was i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8193bfc24ef6>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# one hot encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# predict character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 380 into shape (1,1,10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cibgpLWAFJf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "0c324132-b089-4a3e-d601-9a6ad7b9b8be"
      },
      "source": [
        "# test not in original\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2cceb2bbe2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hello worl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-8193bfc24ef6>\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# one hot encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# predict character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 380 into shape (1,1,10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs2jYVJIFMMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}